% !TeX root = ../ecuaciones-diferenciales.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Clase del 06/03
\chapter{Teorema general de existencia y unicidad}
Vamos a considerar el PVI:
$$
    \begin{cases}
        X' = F(t, X)\\
        X(t_0) = X_0
    \end{cases}
$$
Donde definimos $F$ como:
$$
    F: [a, b] \times \Omega \to \R^d
$$
donde $\Omega \in \R^d$ es abierto y $t_0 \in [a, b]$. Vamos a considerar entonces:
$$
    X(t) = X(t_0) + \int_{t_0}^t X'(s) \d s \text{ y si $X(t)$ cumple el PVI, entonces } X(t_0) + \int_{t_0}^t X'(s) = X(t_0) + \int_{t_0}^t F(s, X(s)) \d s \equiv G(X)(t)
$$
Entonces hay que resolver:
$$
    X = G(X) \text{ con $G :$ espacio de funciones $\to$ espacio de funciones}
$$
Intentamos:
$$
    \begin{cases}
        X_0(t) = X_0\\
        X_{n+1}(t) = G(X_n)(t)
    \end{cases}
$$
Y nuestro objetivo es ver que $X_n$ converge a una función $X$, y además que:
$$
    X_n \to X \implies G(X_n) \to X \text{ es decir, } G \text{ es continua.}
$$
Para ello vamos a ver un par de secciones de recordatorio de análisis. Tras ello escribiremos ciertas propiedades previas para terminar por demostrar el teorema.
\section{Espacio vectorial de funciones.}
En esta sección vamos a hablar de funciones $f: I \to \R$ con $I \subset \R$.
\begin{dfn}[Convergencia puntual y convergencia uniforme]
    Sea $f_n : I \to \R$, $f: I \to \R$.\\
    \begin{enumerate}
        \item $f_n$ converge a $f$ puntualmente (o punto a punto) $\iff f_n(t) \to f(t)\ \forall t \in I$ cuando $n\to \infty$.
        \item $f_n$ converge uniformemente a $f$ si y solo si:
        $$
            \forall \varepsilon > 0,\ \exists N \in \N : |f_n(t) - f(t)| < \varepsilon\ \forall t \in I\ \forall n \geq N
        $$
    \end{enumerate}
\end{dfn}
\begin{obs}
    Algunos comentarios:\\
    \begin{itemize}
        \item En (1), la convergencia se puede expresar formalmente como:
    $$
        \forall \varepsilon > 0,\ \forall t \in I \exists N \in \N : |f_n(t) - f(t)| < \varepsilon\ \forall n \geq N
    $$
    La diferencia principal es que $N$ en (1) depende de $t$ y $ \varepsilon$, pero en (2) $N$ sólo depende de $\varepsilon$.
        \item Claramente, $f_n \xrightarrow[unif]{} f \implies f_n \xrightarrow[pp]{} f$
\end{itemize}
\end{obs}
\begin{eg}[Convergencia de una función]
    Sea $f_n: [0, 1] \to \R$, $f_n(x) = 1+x^n$. Entonces:
    $$
        f_n(x) \xrightarrow[n \to \infty]{}
        \begin{cases}
            1 \text{ si } 0\leq x < 1 \\
            2 \text{ si } x = 1
        \end{cases} \equiv f
    $$
    Entonces es fácil ver que: $f_n \xrightarrow[pp]{} f$\\\\
    Vamos a ver que: $f_n \centernot{\xrightarrow[unif]{}} f$ en $[0, 1]$. Tenemos que hallar $N$ para que se cumpla:
    $$
        |f_n(x) - f(x)| < \varepsilon, n\geq N
    $$
    Si $x < 1$
    $$
        |f_n(x) - f(x)| < \varepsilon = |x^n| < \varepsilon \iff n \log x < \log \varepsilon \text{ (digamos $\varepsilon < 1$) } \implies n > \frac{\log \varepsilon}{\log x}
    $$
    Cuando $x \to 1_-$, $\log x \to 0 \implies \frac{\log \varepsilon}{\log x} \to \infty$, entonces:
    $$
        N \geq \frac{\log \varepsilon}{\log x} \implies N \text{ tiene que depender de }x \implies N = \frac{\log \varepsilon}{\log x} + 1
    $$
    Sin embargo, se puede comprobar que $f_n \xrightarrow[unif]{} f$ en $[0, a]$ con $a < 1$, ya que basta tomar $N = \frac{\log \varepsilon}{\log a}$.
\end{eg}
\begin{obs}
    Otra forma de expresar $f_n \xrightarrow[unif]{} f$ es la siguiente:
    $$
        |f_n(t) - f(t)| < \varepsilon\ \forall t \in I \text{ si } n \geq N
    $$
    Entonces, podemos hablar de:
    $$
        ||f_n - f||_{\infty} = \sup_{t\in I} |f_n(t) - f(t)| \leq \varepsilon
    $$
\end{obs}
\begin{dfn}[Norma infinito]
    Sea $g: I \to \R$. Llamamos \textbf{norma infinito} de $g$ al resultado:
    $$
        ||g||_{\infty} = \sup_{t\in I} |g(t)|
    $$
    que puede ser $+\infty$ si $g$ no está acotada.
\end{dfn}
\begin{pro}[Propiedades de $||\cdot||_\infty$]
    La norma infinito tiene las siguientes propiedades:
    \begin{enumerate}
        \item
        $$
            ||f(t)||_{\infty} = 0 \iff f = 0\ \forall t \text{ en su dominio}
        $$
        \item
        $$
            ||\lambda f||_\infty = |\lambda| ||f||_\infty, \lambda \in \R
        $$
        \item
        $$
            ||f + g||_\infty \leq ||f||_\infty + ||g||_\infty
        $$
    \end{enumerate}
\end{pro}
\begin{proof}
    (1), y (2) son directas, vamos a ver la demostración de (3).\\
    $$
        |f(t) + g(t)| \geq |f(t)| + |g(t)| \geq ||f||_\infty + ||g||_\infty \implies ||f + g||_\infty = \sup_{t \in I} |f(t) + g(t)| \geq ||f||_\infty + ||g||_\infty
    $$
\end{proof}
\begin{obs}
    De la demostración observamos que:
    \begin{itemize}
        \item $ +\infty + a = +\infty$
        \item $ f_n \to_\infty f \iff ||f_n - f||_\infty \to 0 $
    \end{itemize}
\end{obs}
\begin{eg}[Espacios de funciones]
    Algunos espacios de funciones son:
    \begin{enumerate}
        \item
        $$
            B(I) = \left\{ g : I \to \R, g \text{ acotada} \right\}
        $$ es decir, $\exists L_g \in \R : |g(t)| < L_g\ \forall t \in I$\\
        Entonces, $B(I)$ es un espacio vectorial y $||\cdot||_\infty$ es una norma en él.
        %%%%TODO: AÑADIR ÚLTIMO EJEMPLO
    \end{enumerate}
\end{eg}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Clase del 07/03

%%TODO: Añadir dibujos
\begin{eg}[Determinación de convergencia uniforme]
    Sea $f_n$:
    $$
        f_n(x) = \frac{x}{1+n^2x} \text{ en $[0,1]$ }
    $$
    Vamos a ver si converge uniformemente a algo.
    Reescribimos $f_n$ cuando $n \to \infty$:
    $$
        f_n(x) \xrightarrow[n \to \infty]{}
        \begin{cases}
            0 \text{ si } x = 0\\
            0 \text{ si } x > 0
        \end{cases}
    $$
    Por la definición, queremos ver si se cumple:
    $$
        \forall \varepsilon \exists N=N(\varepsilon) : |f_n(x) - f(x)| < \varepsilon\ \forall n \geq N,\ \forall x \in [0,1]
    $$
    Entonces, tenemos que hallar $N$ tal que:
    $$
        \left|  \frac{x}{1+n^2x} \right| < \varepsilon\ \forall n \geq N
    $$
    Distinguimos dos casos:
    \begin{itemize}
        \item Caso $0 \leq x < \varepsilon$:
        $$
            \frac{x}{1+n^2x} \leq x < \varepsilon
        $$
        \item Caso $\varepsilon \leq x \leq 1$:
        $$
            \frac{1}{1+n^2x} \leq \frac{1}{1+n^2\varepsilon} < \varepsilon
        $$
        $$
            \frac{1}{1+n^2x} < \varepsilon \iff n^2\varepsilon > \frac{1}{\varepsilon} - 1 \implies n \geq \sqrt{\frac{(\sfrac{1}{\varepsilon} - 1)}{\varepsilon}} = N(\varepsilon)
        $$
    \end{itemize}
\end{eg}

\begin{th_ex}
    Resolver el ejemplo anterior hallando que $||f_n||_\infty \to 0$.
\end{th_ex}

\begin{pro}[Sucesión de funciones continuas]
    Sean $f_n, f : I \to \R$, tal que $f_n \xrightarrow[unif]{} f$. Si $f_n$ es continua para todo $n \in \N$ entonces $f$ es continua.
\end{pro}
\begin{proof}
    Sea $t_0 \in I$, queremos ver si $f$ es continua en $t_0$.\\
    Decimos que $f$ es continua si:
    $$
        \forall \varepsilon > 0 \exists \delta > 0 : \text{ si } |t-t_0|_{t\in I} < \delta \implies |f(t)-f(t_0)| < \varepsilon
    $$
    Desarrollamos:
    $$
        |f(t) - f(t_0)| = |f(t) -f_n(t) + f_n(t) -f_n(t_0) + f_n(t_0) - f(t_0)| \leq |f(t) - f_n(t)| + |f_n(t_0) - f(t_0)| + |f_n(t) - f_n(t_0)|
    $$
    Como $f_n \xrightarrow[unif]{} f$, $\exists N : |f_n(t) - f(t)| < \sfrac{\varepsilon}{3}$. Fijamos $n = N$, entonces:
    \begin{gather*}
        |f(t) - f_n(t)| < \sfrac{\varepsilon}{3}\\
        |f_n(t_0) - f(t_0)| < \sfrac{\varepsilon}{3}
        |f_n(t) - f_n(t_0)| = |f_N(t) - f_N(t_0)|
    \end{gather*}
    Como $f_N$ es continua:
    $$
        \exists \delta > 0 : |f_N(t) - f_N(t_0)| < \sfrac{\varepsilon}{3} \implies |f(t) - f(t_0)| \leq \frac{\varepsilon}{3}+\frac{\varepsilon}{3}+\frac{\varepsilon}{3} = \varepsilon
    $$
\end{proof}
\begin{cor}
    $$
        \begin{cases}
            f_n \xrightarrow[pp]{} f\\
            f_n \text{ continua } \forall n\\
            f \text{ no continua}
        \end{cases} \implies f_n \centernot{\xrightarrow[unif]{}} f
    $$
\end{cor}

Sin embargo, todo este desarrollo requiere que conozcamos el límite de $f_n$ en caso de que converja. Vamos a ver como podemos comprobar que $f_n$ converge uniformemente sin conocer su (posible) límite.
\begin{dfn}[Sucesión de Cauchy de funciones]
    Sean $f_n : I \to \R$ son una sucesión de Cauchy para $||\cdot||_\infty \iff ||f_n - f_m||_\infty \to 0_{n,m \to \infty}$, es decir:
    $$
        \forall \varepsilon > 0 \exists N : ||f_n - f_m||_\infty < \varepsilon\ \forall n,m \geq N
    $$
\end{dfn}
\begin{pro}[Sucesión de Cauchy y convergencia uniforme]\ref{pro:cauchy-unif}
    Si $\{f_n\}_{n\in\N}$ es una sucesión de Cauchy, entonces:
    $$
        \exists f : I \to R : f_n \xrightarrow[unif]{} f
    $$
\end{pro}
\begin{proof}
    Vamos a ver ciertas afirmaciones:
    \begin{enumerate}
        \item $f_n(x)$ converge $\forall x \in I$. Sea $x\in I$, entonces:
        $$
            |f_n(x) - f_m(x) | \leq \sup_{y\in I} |f_n(y) - f_m(y)| = ||f_n - f_m||_\infty
        $$
        Entonces para $x$ fijado, $\{f_n\}_{n\in\N}$ es una sucesión de Cauchy en $\R \implies f_n(x)$ converge a un punto de $\R$. Llamemos $f(x)$ a ese punto: $f : I \to \R$.
        % TODO: Completar con mis fotos
    \end{enumerate}
\end{proof}
\begin{th_ex}
    El recíproco de la proposición \label{pro:cauchy-unif} también es cierto. Demuéstralo.
\end{th_ex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Clase del 11/03
Debido a que hemos definido la norma $||\cdot||_\infty$, esto nos permite definir una distancia $\d(f, g) = || f -g || _\infty$. Y como hemos definido una norma para el espacio vectorial de funciones, entonces lo llamamos \textit{espacio vectorial normado} y tiene ciertas propiedades.
\begin{pro}
    El espacio vectorial de funciones $(C([a, b]), ||\cdot||_\infty$ es completo, es decir, toda sucesión de Cauchy converge a un elemento de ese espacio.
\end{pro}
\begin{proof}
    Sea ${f_n}_{n\in N}$ una suciesión de funciones del espacio vectorial.
    \begin{enumerate}
        \item Cauchy $\implies$ converge uniformemente $( a f)$.
        \item
            $$
                \begin{cases}
                    f_n \text{ cont.}\\
                    f_n \xrightarrow[unif]{} f
                \end{cases} \implies f \text{ cont. (y $||f_n - f|| \to 0$)}
            $$
    \end{enumerate}
\end{proof}
\begin{pro}[Limite de la integral de una sucesión de funciones]
    Sean $f_n,\ f :\ [a, b] \to \R$ integrables y tal que $f_n\xrightarrow[unif]{}f$ en $[a, b]$, entonces:
    $$
        \int_a^b f_n \to \int_a^b f
    $$
\end{pro}
\begin{cor}
    $$
        \begin{cases}
            f_n \in C_1([a,b])\\
            f_n \xrightarrow[unif]{} f (\implies f\in C_1([a, b]))
        \end{cases} \implies \int_a^b f_n \to \int_a^b f
    $$
\end{cor}
\begin{proof}
    $$
        \left|\int_a^b f_n - \int_a^b f \right| = \left|\int_a^b (f_n-f) \right|  \leq \int_a^b |f_n(x) - f(x)| \d x \leq \int_a^b ||f_ n - f||_\infty = ||f_n - f||_\infty \cdot (b - a) \to 0
    $$
    Es decir, $$|f_n(x) - f(x)| < ||f_n - f||_\infty$$
\end{proof}

\section{Series de funciones}
Como recordatorio:
\begin{enumerate}
    \item Sea $a_n \in \R$, $\sum a_n$ converge $\iff s_n = \sum_{m=0}^n a_m \iff \{s_n\} \text{ es de Cauchy} \iff \sum_{k=m}^n a_k \text{ converge a } 0$.
    \item Sea $a_n \geq 0$, si:
    $$
        \exists \lim_{n \to \infty} \frac{a_{n+1}}{a_n} < 1 \implies \sum a_n \text{ converge}
    $$
\end{enumerate}
Consideremos ahora $f_m : I \to \R$, y sea $S_n(x) = \sum_{k=1}^n f_k(x)$  entonces podemos definir:
$$
    \left(\sum f_n\right) \xrightarrow[pp]{} g \implies  S_n(x)\xrightarrow[pp]{} g(x)\ \forall x \in I \text{ pues las sumas parciales convergen.}
$$
$$
    \left(\sum f_n\right) \xrightarrow[unif]{} g \implies S_n(x) \xrightarrow[unif]{} g(x)\ \forall x \in I.
$$
\begin{pro}[Criterio de Cauchy]
    $$
        \sum f_n \xrightarrow[unif]{} g \iff \{S_n\} \text{ es sucesión de Cauchy para $||\cdot||_\infty$}
    $$
\end{pro}
\begin{proof}
    Se deja al lector. No la vimos en clase.
\end{proof}
\begin{pro}[Criterio de Weierstrass]
    Sea $f_n I \to \R$ y supongamos que $||f_n||_\infty \leq M_n \in \R$  y que $\sum_{n=1}^\infty < \infty$. Entonces:
    $$
        \sum f_n \text{ converge uniformemente en $I$ y su límite se denota } \sum_{n=1}^\infty f_n(x)
    $$
\end{pro}
\begin{proof}
    Partimos de:
    $$
        \sum f_n \xrightarrow[unif]{} g = S_n \xrightarrow[unif]{} g \text{ donde } S_n(x) = \sum_{k=1}^n f_k(x)
    $$
    además:
    $$
        S_n \xrightarrow[unif]{} g \iff \{S_n\} \text{ es Cauchy en $|| \cdot ||_\infty$, es decir } ||S_n - S_m || \to 0
    $$
    y como,
    $$
        S_n(x) - S_m(x) = \sum_{k=m+1}^n f_k(x)
    $$
    tomando $||\cdot||_\infty$:
    $$
        ||S_n(x) - S_m(x)||_\infty = ||\sum_{k=m+1}^n f_k(x)||_\infty \leq \sum_{k=m+1}^{n} ||f_k|| \leq \sum_{k=m+1}^{n} M_k \to 0 \text{ pues $M_k$ converge.}
    $$
\end{proof}
\begin{eg}[Convergencia uniforme por el criterio de Weierstrass]
    Sea $\{f_n\}_{n\in\N} = \sum_{n=1}^\infty x^n$, ¿converge uniformemente en $|x| \leq r < 1$?\\
    $$
        f_n(x) = x^n,\ I = [-r, r] \implies |x^n| \leq |x|^n \leq r^n
    $$
    Además, sabemos que $\sum r^n$ converge si $r<1$.
    Por el criterio de Weierstrass:
    $$
        \sum x^n \text{ converge uniformemente en } |x| \leq r, \text{ como } S_n=\sum_{k=1}^n x^k \text{ es continua y } S_n(x) \xrightarrow[unif]{} \sum_{m=1}^\infty x^m
    $$
    entonces:
    $$
        f(x) = \sum_{m=1}^{\infty} x^m \text{ es continua en } I = [-r, r] \text{ y por tanto en } |x| < 1
    $$
\end{eg}

\begin{eg}[Convergencia uniforme de la función Zeta de Riemann]
    Sea $\sum_{n=1}^\infty \frac{1}{n^x}$, veamos que converge uniformemente en $x \geq a > 1$.
    %%TODO: Completar. Pedir a Santorum.
\end{eg}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Clase del 12/03
\begin{eg}[Convergencia de la serie $\sum_{n=1}^\infty \sfrac{\cos(3^n x)}{2^n}$]
    Vamos a ver que converge uniformemente en $\R$.
    $$
        \left|\left|\frac{\cos(3^n x)}{2^n}\right|\right|_\infty = \frac{1}{2^n} \text{ y } \sum \frac{1}{2^n} < \infty
    $$
    entonces, por el criterio de Weierstrass la serie converge.\\
    Como curiosidad, esta función fue el primer ejemplo de función continua pero no derivable en ningún punto. Se conoce como función de Weierstrass.
\end{eg}
\begin{pro}[De la convergencia uniforme de una función y su derivada]\ref{pro:deriv-impl-con}
    Sean $f_n : [a, b] \to \R$ con $f_n \in C^1$. Supongamos que:
    $$
        f_n' \xrightarrow[unif]{} g \text{ y que } \exists t_0 \in [a, b] : f_n(t_0) \text{ converge.}
    $$
    Entonces:
    $$
        \exists f : [a, b] \to \R \text{ con } f\in C^1\ :\
        \begin{cases}
            f_n \xrightarrow[unif]{} f\\
            f' = g
        \end{cases}
    $$
\end{pro}
\begin{th_ex}
    La demostración de la proposición \label{pro:deriv-impl-con} se deja como ejercicio.
\end{th_ex}
\begin{eg}[Técnica habitual para probar convergencia uniforme de funciones]
    Para comprobar que $f_n$ converge uniformemente a algo, se puede pasar a una serie de funciones (ya que tenemos más herramientas para probar convergencia de funciones):
    \begin{gather*}
        g_1 = f_1\\
        g_2 = f_2-f_1\\
        g_n = f_n - f_{n-1}
    \end{gather*}
    Entonces:
    $$
        f_n = \sum_{m=1}^n g_m \text{ es decir, } f_n \text{ converge uniformemente } \iff \sum g_m \text{ lo hace.}
    $$

\end{eg}

\begin{obs}
    Veamos ciertos aspectos de lo que hemos visto:
    \begin{itemize}
    \item Todo lo visto para funciones con valores en $\R$ es válido para funciones $I \to \C$ ($| \cdot |$ = módulo)\\
    \item También para funciones $I \to \R^d$ e $I \to \C^d$. Si tenemos el vector:
    $$
        \mx{v_1\\v_2\\\vdots\\v_d} \implies |v| = \sum_{j=1}^d |v_j| \text{ (o cualquier otra norma)}
    $$
    \end{itemize}

\end{obs}

\section{Teorema de existencia y unicidad}
Vamos a ver enunciado preciso, demostración y algunos usos. Comenzamos con un previo al teorema.\\
\begin{dfn}[Función Lipschitz]
    Sea $A \in \R^m, f: A \to \R^d$. Diremos que $f$ es \textbf{Lipschitz} $\iff$
    $$
        \exists L \in \R : |f(x) - f(y)| \leq L \cdot |x - y|\ \forall x,y \in A
    $$
    %%TODO: Incluir visualización con el cono?
\end{dfn}
\begin{obs}
    Una función Lipschitz siempre es continua. Para verlo basta coger $\varepsilon = \sfrac{L}{\delta}$ en la definición de continuidad.
\end{obs}
\begin{pro}[Derivadas parciales acotadas implica Lipschitz]
    Sea $\Omega \in \R^m$ un abierto, $f:\Omega \to \R^d$ con $f \in C^1$.
    \begin{enumerate}
        \item Si $\Omega$ es convexo y $\exists C \in \R$ tal que:
        $$
            \left| \frac{\partial f_i}{\partial x_j} (x) \right| \leq C\ \forall x\in \Omega; \forall i = 1, \ldots, d; \forall j = 1, \ldots, m \implies f \text{ es Lipschitz.}
        $$
        \item Si $A \in \Omega$ es compacto y convexo, $f$ es Lipschitz en $A$.
    \end{enumerate}
\end{pro}
\begin{proof}
    Vamos a ver la demostración de la proposición:
    \begin{enumerate}
        \item
    Como $\Omega$ es convexo, consideramos $\gamma: [0,1] \to \R^m$ con $\gamma(t) = x + t(y-x),\ 0\leq t\leq 1$.
    $$
        f(y) - f(x) = f(\gamma(1)) - f(\gamma(0)) = \int_0^1 \Dd{t} (f(\gamma(t))) = \int_0^1 \sum_{j=1}^m \frac{\partial f}{\partial x_j}(\gamma(t)) \gamma_j'(t) \d t
    $$
    Tomando valores absolutos llegamos a:
    \begin{align*}
        |f(x) - f(y)| &\leq \sum \int_0^1 \left(\left| \frac{\partial f}{\partial x_j} \gamma(t) \right| \cdot |y_j - x_y|\right) \d t\\
                     (*) & \leq \int_0^1 C\cdot d \sum |y_j - x_j| \d t =  C\cdot d |y -x| = L |y-x|
    \end{align*}
    $(*)$ se cumple pues $\left|\sfrac{\partial f}{\partial x_j}\right| = \sum_{k=1}^d \left| \frac{\partial f_k}{\partial x_j} \right| \leq C\cdot d$ por hipótesis.
    \item Igual que en 1, usando que al ser $A$ compacta y $\frac{\partial f_i}{\partial x}$ continua, $\exists C$ como en 1.
\end{enumerate}
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Clase del 13/03
\begin{cor}
    Sea $\Omega \in \R^m$ abierto, $f: \Omega \to \R^d$ con $f \in C^1$. Entonces $f$ es \textit{localmente} Lipschitz, es decir, si $x_0 \in \Omega$ y $\bar{\B}(x_0, \delta) \in \Omega$, se tiene que $f$ es Lipschitz en $\bar{\B}(x_0, \delta)$.
\end{cor}
\begin{proof}
    Sea $A = \bar{\B}(x_0, \delta)$ en un compacto, convexo en $\Omega$.
\end{proof}
\begin{obs}
    Estos van a ser los casos que usemos pero:
    \begin{enumerate}
        \item $f(x) = |x|$ es Lipschitz en $\R$.
        \item Si $f(x) = \int_0^x g(u) \d u$, con $g$ integrable y acotada en $[-a, a]$, es Lipschitz.
        $$
            |f(y)-g(y)| = \left|\int_x^y g(u) du\right| \leq ||g||_\infty |y-x|
        $$
    \end{enumerate}
\end{obs}
\subsection{Teorema de existencia y unicidad global}

\begin{thm}[Unicidad y existencia global]
    Sea $F: [a, b] \times \R^d \to \R^d$ continua, tal que:
    $$
        |F(t, X) - F(t, Y)| \leq L|X - Y|\ \forall t \in [a, b]
    $$
    es decir, $F$ es Lipschitz en $t\in[a, b]$. Sea $t_0 \in [a, b]$ u $X_0 \in \R^d$, entonces:
    $$
        \exists X:[a, b] \to \R^d \text{ con } X\in C^1 : \begin{cases}
            X'(t) = F(t, X(t))\ \forall t\in [a, b]\\
            X(t_0) = X_0
        \end{cases}
    $$ esa $X(t)$ es única.
\end{thm}
\begin{proof}
    Dividiremos en la prueba en distintos pasos.\\
    \begin{enumerate}
        \item \ref{proof:primera-parte-unic} Sea $h>0$, con $h < \sfrac{1}{L}$ ($Lh < 1$). Entonces, veremos que existe $X(t) \in C^1$ definida sobre $I = [t_0-h, t_0+h]\cap[a, b]$ y tal que:
        $$
            \begin{cases}
                X' = F(t, X) \text{ en } I\\
                X(t_0) = X_0
            \end{cases}
        $$

        \begin{enumerate}
        \item Definimos recursivamente:
            $$
                \begin{cases}
                    X_0(t) = X_0,\ t\in[a, b]\\
                    X_{n+1} = X_0 + \int_{t_0}^t F(s, X_n(s)) \d s,\ n \geq 0
                \end{cases}
            $$
        \item Vamos a ver que $X_n$ es continua y está definida para todo $t \in [a, b]$ por inducción en n.\\
            El caso base se cumple ($X_0$ es continua). Vamos a ver si es cierto para cualquier $n$.
            \begin{gather*}
                X_n \text{ continua } \implies F(s, X_n(s)) \text{ es continua por composicion de continuas } \implies \\
                \implies \int_{t_0}^t F(s, X_n(s)) \d s \text{ continua } \implies X_{n+1} \text{ continua.}
            \end{gather*}
        \item \ref{proof:ninf}Ahora vamos a ver que $\ninf{X_{n+1} - X_n} \leq (Lh)^n \ninf{X_1 - X_0}$ Por inducción:
        $$
            \text{Para $t \in I$, }
            X_{n+1}(t) - X_n(t) = \int_{t_0}^t F(s, X_{n}(s)) \d s - \int_{t_0}^t F(s, X_{n-1}(s)) \d s
        $$
        entonces:
        \begin{align*}
            |X_{n+1}(t) - X_n(t)| &\leq \left|\int_{t_0}^t |F(s, X_{n}(s)) \d s - F(s, X_{n-1}(s))| \d s\right|\\
            \text{($F$ es Lipschitz) }&\leq \left| \int_{t_0}^t |X_n(s) - X_{n-1}(s)| \right|\\
            &\leq \left| \int_{t_0}^t L \ninf{X_n - X_{n-1}} \d s \right|
        \end{align*}
        Entonces:
        $$
            \left| \int_{t_0}^t L \ninf{X_n - X_{n-1}} \d s \right| = L \ninf{X_n - X_{n-1}} |t-t_0| \leq Lh \ninf{X_n - X_{n-1}}
        $$
        Tomando $\max_{t\in I}$, llegamos a $\ninf{X_{n+1} - X_n} \leq (Lh)^n \ninf{X_1 - X_0}$.
        \item Veamos que $X_n^(t)$ converge uniformemente en $I$.\\
        Sea $D_n = X_n - X_{n-1}$, entonces $X_n = X_0 + \sum_{i=1}^n D_i$. Por lo tanto $X_n$ converge $\iff \sum D_n$ lo hace.\\
        Por \label{proof:ninf}:
        $$
            \ninf{D_n} \leq (Lh)^{n-1} \ninf{D_1}
        $$
        Si $Lh < 1$ la serie $\sum (Lh)^k$ converge por el Criterio de Weierstrass. Por tanto:
        $$
            \sum D_n \text{ converge uniformemente en } I \implies X_n\xrightarrow[unif]{} X
        $$
        \item Veamos que $X(t) = X_0 + \int_{t_0}^t F(s, X(s)) \d s,\ t\in I$.\\
        Pasamos al limite en:
        $$
            X_{n+1} = X_0 + \int_{t_0}^t F(s, X_{n}(s)) \d s
        $$
        Para ello necesitamos que ambos sumandos converjan uniformemente. Tenemos que ver:
        $$
            F(t, X(t)) \xrightarrow[unif]{} F(t, X(t))
        $$
        Es decir:
        \begin{gather*}
            |F(t, X(t)) - F(t, X_n(t))| \leq L |X(t) - X_n(t)| \leq L\ninf{X-X_n}\\
            \ninf{F(\cdot, X(\cdot)) - F(\cdot, X_n(\cdot))} \leq L \ninf{X - X_n} \to 0 \implies\\
            \implies F(t, X_n(t)) \xrightarrow[unif]{} F(t, X(t)) \implies \lim_{n\to \infty} \int_{t_0}^t F(s, X_n(s)) \d s = \int_{t_0}^t F(s, X(s)) \d s
        \end{gather*}
        Y por tanto:  $X(t) = X_0 + \int_{t_0}^t F(s, X(s)) \d s,\ t\in I$
        \item Como $X(t) = X_0 + \int_{t_0}^t F(s, X(s)) \d s,\ t\in I$. Se cumple que:\\
            \begin{itemize}
                \item $X(t_0) = X_0 + 0$
                \item $X'(t_0) = \left( \int_{t_0}^t F(s, X(s)) \d s \right) = F(t, X(t))$
            \end{itemize}
        \item Veamos que $X$ es única, es decir, que si $Y: I \to \R^d$, con $Y \in C^1$ satisface:
        $$
                \begin{cases}
                    Y'(t) = F(t, Y(t)),\ t\in I\\
                    Y(t_0) = X_0
                \end{cases}
        $$ entonces $X(t) = Y(t) \ \forall t \in I$\\
        $$
            \text{Como } Y(t) = X_0 + \int_{t_0}^t F(s, Y(s)) \d s \implies Y(t) - X(t) = \int_{t_0}^t [F(s, Y(s)) - F(s, X(s))] \d s
        $$
        Supongamos sin pérdida de generalidad que $t_0 < t$. Entonces:
        %\begin{gather*}
        %    | Y(t) - X(t) | &= \left| \int_{t_0}^t [F(s, Y(s)) - F(s, X(s))] \d s \right| \leq \int_{t_0}^t |F(s, Y(s)) - F(s, X(s))| \d s\\
        %                    &\leq \int_{t_0}^t L |Y(s) - X(s)| \d s \leq L \int_{t_0}^t \ninf{Y - X} = L \ninf{Y - X} (t-t_0) \leq Lh \ninf{Y - X}
        %\end{gather*}
        Entonces:
        $$
            \ninf{Y-X} \leq Lh \ninf{Y-X}
        $$
        Pero $Lh < 1$, por tanto $\ninf{Y-X} = 0 \implies Y(t) = X(t)$. Es decir, nuestra solución es única
    \end{enumerate}
        \item La solución encontrada en \label{proof:primera-parte-unic} se puede extender (\textit{prolongar}) a todo el intervalo $[a, b]$.
        Supongamos que podemos extender la solución a $[\alpha, \beta]$ y ese $\beta$ es el máximo con esa propiedad, entonces vamos a ver que $\beta = b$.\\\\
        Si $\beta < b$, sea $h = \sfrac{1}{2L}$, sea $\hat{t}_0 = \beta - \sfrac{h}{2}$ y $\hat{X}_0 = X(\hat{t}_0)$, por \label{proof:primera-parte-unic} aplicado da una solución $\hat{X}$ definida en $[\hat{t}_0 - h, \hat{t}_0 + h] \cap[a, b]$.
        Sea:
        $$
            X_{nueva}(t) =
            \begin{cases}
                X(t),\ t\leq \beta\\
                \hat{X}(t),\ t\geq \hat{t}_0 - h
        \end{cases}
        $$
        Como $X$ y $\hat{X}$ están definidas en todo $[\hat{t}_0 - h, \hat{t}_0 + h]$ y cumplen la EDO y $X(\hat{t}_0) = \hat{X}(\hat{t}_0)$, por la unicidad de  \label{proof:primera-parte-unic}: $X(t) = \hat{X}(t) $ para $t \in [\hat{t}_0 - h, \hat{t}_0 + h] \cap[a, b]$.
        Entonces $X_{nueva}$ es solución de la EDO hasta $\beta + \frac{h}{L}$ (o $\min(\beta + \sfrac{h}{2}, b)$), es decir, para $t < \beta$ usamos $X$ y para $\beta - \sfrac{h}{2} < t$ usamos $\hat{X}$. Entonces, con $X_{nueva}(t_0) = X_0 = X(t_0)$ lo que contradice nuestra hipótesis de $\beta < b$.\\
        Para demostrar que podemos prolongar la solución también a $a$ se repite el argumento.
        \item Hemos visto que la solución existe en todo el intervalo $[a, b]$. Vamos a ver también que tenemos unicidad en $[a, b]$.
        \begin{enumerate}
        \item C es no vacío:
        $$C = \{t\in [a, b] : X(t) = Y(t)\}$$ entonces $C \neq \varnothing$ ya que $t_0\in C$.
        \item C es cerrado en $[a, b]$:\\
        Basta ver que si $t_n \in C\ \forall n$ y $t_n \to t$ entonces $t \in C$. Si $t_n \to t$
        $$
            X(t_n) = Y(t_n) \implies X(t) = Y(t) \implies t \in C \implies C \text{ es cerrado.}
        $$
        \item C es abierto en $[a, b]$:\\
        Tenemos que ver que $\forall \bar{t}\in C \exists \delta > 0$ tal que $(\bar{t} - \delta, \bar{t} + \delta) \cap [a, b] \subset C$. Es decir:
        $$
            X(\bar{t}) = Y(\bar{t}) \text{ pues }\bar{t} \in C
        $$
        Además, $X, Y$ son soluciones de:
        $$
        \begin{cases}
            Z' = F(t, Z)\\
            Z(\bar{t}) = X(\bar{t}) = Y(\bar{t})
        \end{cases}
        $$
        Por unicidad local en $[\hat{t}_0 - h, \hat{t}_0 + h] \cap[a, b]$, $X=Y$ en ese intervalo.
        \item Veamos que $C$ es el total:\\
        Como $C$ es abierto en $[a,b]$, cerrado en $[a, b]$,  $C \neq \varnothing$ y $[a, b]$ es conexo, entonces $C = [a, b]$.
        \end{enumerate}
    \end{enumerate}

\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Clase del 14/03
\begin{cor}
    Sean $\A : [a, b]\to \R^{d\times d}$, $B : [a, b]\to\R^{d\times d}$ continuas, $t_0 \in [a, b]$, $X_0 \in \R^d$. Entonces existe una solución (y es única) $X : [a, b] \to \R^d$ de:
    $$
        \begin{cases}
            X'(t) = \A(t) X(t) + B(t)\\
            X(t_0) = X_0
        \end{cases}
    $$ %TODO: Sustituir las x de abajo por negrita
    Además, si $p, q, r :[a, b] \to \R$, $t_0 \in [a, b]$, $x_0, y_0 \in \R$, entonces existe solución y es única $x : [a, b] \to R$ del problema de valores iniciales:
    $$
        \begin{cases}
            x'' + p(t) x' + q(t) x = r(t),\ \forall t \in [a,b]\\
            x(t_0) = x_0
            x'(t_0) = y_0
        \end{cases}
    $$
\end{cor}
\begin{proof}
    Sabemos que $X'(t) = F(t, X(t))$, $F(t, X) = \A(t) X +  B(t)$.
    $$
        \A(t) = \mx{a_{11}(t) & \cdots & a_{1d}(t)\\
            \vdots &    \ldots & \vdots \\
            a_{d1}(t) & \cdots & a_{dd}(t)}
    $$
    Como $a_{ij} : [a,b] \to \R$ es continua, entonces $F$ es continua. Queremos ver ahora que $F$ satisface una condicion de Lipschitz uniforme en t. Es decir, tenemos que ver que:
    $$
        \exists k : \sum_{j=1}^d |a_{ij}(t)| \leq k ,\ \forall t \in [a, b]
    $$
    Si la suma es una función continua en $[a, b] \implies$ f es acotada. Entonces:
    $$
        |F(t, Y) - F(t, X)| = |A(t) (Y - X)|
    $$
    Además:
    $$
        A(t)(Y-X) = \sum_{j=1}^d a_{ij}(Y_j - X_j)
    $$
    Por tanto:
    $$
     |F(t, Y) - F(t, X)| \leq \sum_{j=1}^d |a_{ij}(t)||Y_j-X_j| \leq \left(\sum_{j=1}^d |a_{ij}(t)|\right) |Y-X| \leq k |Y - X|
    $$
    Y por tanto, es Lipschitz y cumple las condiciones del teorema de existencia y unicidad.\\\\
    En el caso de la EDO: $x'' + p(t) x' + q(t) x = r(t)$ entonces es como si tuviéramos el sistema:
    $$
        \mx{x\\y}' = \mx{0 & 1 \\ -q(t) & -p(t)} \mx{x\\y} + \mx{0\\r}
    $$
    Y por consecuencia de lo que acabamos de ver, tomando:
    $$
        \A(t) = \mx{0 & 1 \\ -q(t) & -p(t)} \text{ y } \B(t) = \mx{0\\r}
    $$
    Existe solución y es única.
\end{proof}

\begin{obs}
    Aunque no lo hemos enunciado así, hemos demostrado:
    $$
        \exists ! X : I \to \R^d \text{ tal que } X(t) = X_0 + \int_{t_0}^t F(s, X(s)) \d s,\ t \in I
    $$
    Donde llamamos $I = [t_0 - h, t_0 + h] \cap [a, b]$ y $G(x) = X_0 + \int_{t_0}^t F(s, X(s)) \d s$\\
    Con $G : \mathcal{C}(I) \text{ funciones continuas en } I \to C(I)$ y $G(X) = X$. Es decir, $G$ tiene un único \textbf{punto fijo}. Se puede ver usando:
    $$
        \ninf{G(X) - G(Y)} \leq \alpha \ninf{X - Y} \text{ con } \alpha = Lh < 1
    $$
\end{obs}

\section{Teorema de existencia y unicidad local}

\begin{thm}[Teorema de existencia y unicidad local]\label{thm:exist-unic-local}
    Sea $\Omega \in \R^d$ abierto, $F:[a, b] \times \Omega \to \R^d$ continua y uniformemente Lipschitz en $t$, es decir:
    $$
        \exists L : |F(t, X) - F(t, Y)| \leq L |X - Y| \ \forall t \in [a, b],\ \forall X, Y \in \Omega
    $$
    Sean $t_0 \in [a, b]$, $X_0 \in \Omega$. Si $\delta$ es suficientemente pequeño, entonces:\\\\
    Existe $X:I \to \Omega$, $I = [t_0 - \delta, t_0 + \delta] \cap [a, b]$ con $X \in C^1$, que cumple:
    $$
    \begin{cases}
        X'(t) = F(t, X(t))\ \forall t \in I\\
        X(t_0) = X_0
    \end{cases}
    $$
    Y esa solución es única.
\end{thm}
\begin{proof}
    La demostración es como en el caso $\Omega = \R^d$. Definimos:
    $$
        \begin{cases}
            X_0(t) = X_0 \text{ y, recursivamente, }\\
            X_n+1(t) = X_0 + \int_{t_0}^t F(s, X_n(s)) \d s
        \end{cases}
    $$
    Para que las $X_n$ estén bien definidas hay que asegurarse de que $X_n(t) \in \Omega\ \forall t \in I$. Tenemos que ver cómo elegir $\delta$ para que eso ocurra.
    \begin{enumerate}
        \item
        $$
            \begin{cases}
                \Omega \text{ abierto}\\
                X_0 \in \Omega
            \end{cases} \implies \exists r > 0 : \bar{\B}(X_0, r) \subset \Omega
        $$
        Como $[a, b] \times \bar{\B}(X_0, r)$ es compacto, y $\left. F \right|_{[a, b]\times\bar{\B}}$ es continua, es deicr, $\exists M(r) : |F(t, X)| \leq M$ si $t \in [a, b$ y $X \in  \bar{\B}(X_0, r)$
        \item Si $\delta < \frac{r}{M}$ entonces:
        $$
            X_n(t) \in \bar{\B}(X_0, r) \text{ si } t\in I,\ n \in \N
        $$
        Se demuestra por inducción sobre $n$. Para $n=0$ tenemos $X_0(t) = X_0$ y por tanto se cumple. Veamos el paso inductivo.\\
        \begin{gather*}
            |X_{n+1}(t) - X_0 | = \left| \int_{t_0}^t F(s, X_n(s)) \d s \right| \leq \int_{t_0}^t |F(s, X_n(s))| \d s\\ \leq \int_{t_0}^t M\d s = M(t - t_0) \leq M \delta \leq r \implies \delta < \frac{r}{M}
        \end{gather*}
    \end{enumerate}
    Con esto demostramos que las $X_n$ están bien definidas. Ahora simplemente se procede como en el caso global para demostrar convergencia, para lo que se necesita $L\delta < 1$. Es decir, vamos a necesitar que $\delta < \min (\sfrac{r}{M(r)},\ \sfrac{1}{L})$
\end{proof}
\begin{pro}[Regularidad de las soluciones]
    Con las hipótesis del Teorema de existencia y unicidad local, si $F$ es $C^m ([a,b]\times\Omega)$ entonces:
    $$
        X \in C^{m+1}(I)
    $$
\end{pro}
\begin{proof}
    Por inducción sobre $m$:\\
    Caso $m=1$ $\implies X$ es $C^2$.\\
    $X$ es $C^1$ por la proposición. $X'(t) = F(t, X(t)) \implies X' \in C^1$ por la regla de la cadena, es decir, $X' \in C^1 \implies X \in C^2$. Se sigue la prueba recursivamente.
\end{proof}

\subsection{Prolongabilidad de soluciones}
En el caso de existencia y unicidad global veíamos que si teníamos una solución definida en un intervalo $I$ podíamos extenderlo a todo un intervalo $[a, b]$ simplemente por unicidad de la solución. Vamos a ver que en el caso de existencia y unicidad local puede que no lleguemos a cubrir todo el intervalo $[a, b]$ debido a que el $\delta$ que define la bola abierta donde existe la solución no es fijo y puede ir haciéndose cada vez más pequeño.

\begin{pro}[Intervalo máximo de existencia-unicidad local]
    Con las misma hipótesis del teorema de existencia y unicidad local (véase el teorema \ref{thm:exist-unic-local}), existe un intervalo máximo en el que está definida la solución. Ese intervalo es uno de los siguientes tipos:
    $$[a, b],\ [a,\beta),\ (\alpha, b],\ (\alpha, \beta)
    $$
    Donde $a \leq \alpha < \beta \leq b$.\\
    Además, si es de la forma $*,\ \beta)$ entonces $\forall k \in \Omega$ compacto $\exists t_K < \beta$ tal que $X(t)\not \in K,\ t_K < t < \beta$. Algo similar pasa para los intervalos de la forma $(\alpha, *$
\end{pro}
\begin{obs}
    En palabras más simples y menos precisas, el enunciado establece que:
    $$
        X(t) \xrightarrow[t\to\beta]{} \delta \Omega \text{ o } |X(t)| \xrightarrow[t\to\beta]{} \infty
    $$
\end{obs}
\begin{eg}[No unicidad formalmente]
    Retomando el ejemplo \label{eg:no-unic}:
    $$
        \begin{cases}
            \mbf{x}' = \mbf{x}^{\sfrac{2}{3}}\\
            x(0) = 0
        \end{cases}
    $$
    Se ve que $F(t, \mbf{x})$ no es Lipschitz cerca del $0$, (en $0$ la derivada es $\infty$). De hecho podemos encontrar dos soluciones:
    \begin{gather*}
        x_1(t) = 0\\
        x_2(t) = at^3,\ a \neq 0
    \end{gather*}
\end{eg}

\begin{eg}[Existencia local e intervalo maximal]
    Sea el PVI:
    $$
        \mbf{x}' = \mbf{x}^2,\ t\in[-1, 1]\\
        x(0) = 1
    $$
    Resolviendo el PVI hallamos:
    $$
        x(t) = \frac{1}{1-t} \text{ que sólo existe si t < 1}
    $$
    Por tanto el intervalo de existencia es: $[-1, 1)$.\\\\
    Vemos que si no nos restringiéramos inicialmente a $[a, b] = [-1, 1]$, está definida en $(-\infty, 1)$. Decimos que éste es su intervalo maximal.\\
    Vemos que $F(t, \mbf{x})$ no es Lipschitz en $x\in\R$ pues $\frac{\partial F}{\partial x} = 2\mbf{x}$ no está acotada en $\R$, pero si en $[-A, A]$ para $A \in \R^+$. Es decir, es Lipschitz uniformemente en $t$ si la restringimos a $\R \times [-A, A]$ (la constante de Lipschitz es $L = 2A$).
\end{eg}
\begin{obs}
    Para el caso autónomo se pueden tomar $[a, b]$ cada vez más grandes hasta que llenen $\R$.
\end{obs}

\begin{eg}[Aplicación del teorema de existencia y unicidad en $\mbf{x}'' = \cos(t\mbf{x})$]
    Sea el PVI:
    $$
        \begin{cases}
            \mbf{x''} = \cos(t\mbf{x})\\
            x(0) = 0\\
            x'(0) = 0
        \end{cases}
    $$
    Pasamos a forma matricial:
    $$
        (y = x')\ \mx{x\\y}' = \mx{y\\ cos(tx)} = F(t, X)
    $$
    Entonces vamos a ver si podemos aplicar el teorema de existencia y unicidad:
    $$
        \frac{\partial F}{\partial y} = \mx{1\\0},\ \frac{\partial F}{\partial x} = \mx{0\\ -t\sin(tx)}
    $$
    Donde vemos que ambas derivadas parciales están acotadas si y sólo si $t$ lo está. Por tanto, en $[a, b] \times \R^2$, las derivadas parciales de $F$ con respecto a $x$ e $y$ están acotadas, lo que quiere decir que se puede utilizar el Teorema de existencia y unicidad global.
    $$
        \left| \frac{\partial F}{\partial y} \right| \leq 1,\ \left| \frac{\partial F}{\partial x} \leq \max(|a|, |b|)  \right|
    $$
    Por tanto, $\exists ! $ sol $ \in [a, b]$. Entonces, para $n \in N$ podemos definir:
    $$
        [a, b] = [-n, n] \implies \exists ! X_n: [-n, n] \to \R^2
    $$
    Además:
    $$
        \left. X_{n+1}\right|_{[-n, n]} = X_n \text{ por unicidad} \implies \exists ! X: \R \to \R^2
    $$
    Y por tanto existe solución $\forall t\in \R$ para el PVI.
\end{eg}
\begin{eg}[Existencia y unicidad de solución del péndulo simple]
    La ecuación del péndulo simple (sin resistencia del aire) sigue el PVI:
    $$
        \begin{cases}
            \theta'' = -\frac{g}{l} \sin \theta\text{, con $l$ la longitud del péndulo}\\
            \theta(0) = \theta_0\\
            \theta'(0) = v_0
        \end{cases}
    $$
    Donde podemos ver que tiene solución en $t\in\R$ y es única. Pasando a sistema:
    $$
        \mx{\theta\\v}' = \mx{v\\-\frac{g}{l}\sin\theta} = F(t, \theta, v)
    $$
    Si hallamos sus derivadas parciales:
    \begin{gather*}
        \frac{\partial F}{\partial v} = \mx{1\\0}\\
        \frac{\partial F}{\partial \theta} = \mx{0\\-\frac{g}{l}\cos\theta}
    \end{gather*}
    Donde ambas están acotadas en $[\theta, v]^T\in\R^2$ y por tanto existe solución y es única.
\end{eg}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Clase del 20/03
\section{Dependencia de parámetros.} %%Nuevo tema?

Consideremos una ecuación diferencial como: $X'(t) = F(t, X(t)),\ X(t_0) = \xi$, donde $X(t, \xi)$ es "la" solución para cada valor de $\xi$ (supondremos que se cumplen las hipótesis necesarias para asegurar existencia y unicidad).\\
En ocasiones nos va a interesar saber si cuando se mueve $\xi$ ligeramente, la nueva solución es similar a la original o no.\\
Por otro lado, podría ser que $F$ dependiera de un parámetro $\lambda$, es decir:
$$
    \begin{cases}
        X'(t) = F(t, X(t), \lambda)\\
        X(t_0) = \xi
    \end{cases} \text{ con solución } X(t, \xi, \lambda)
$$
Nos volvemos a preguntar si cuando se mueve $\lambda$ ligeramente, la nueva solución es similar a la original o no.\\\\
\begin{obs}
    Podría parecer que en ambos casos la pregunta es la misma, pero existe un procedimiento para transformar la segunda en la primera de la siguiente manera, definiendo una $Y(t)$:
    $$
        Y(t) = \mx{X(t)\\\lambda} \implies Y' = \mx{X'\\0} = \mx{F(t, X, \lambda)\\0} = G(t, Y) \text{ y } Y(t_0) = \mx{\xi, \lambda}
    $$
\end{obs}
Por último, supongamos que tenemos $\hat{F}(t, X) \approx F(t, X)$, donde $\hat{F}$ es una aproximación de $F$. Para cada función con su respectivo PVI hallaríamos una solución, ($\hat{X}$ resuelve $\hat{F}$ y $X$ resuelve $F$). Nos preguntamos entonces si $\hat{X} \approx X$.

\begin{eg}[Motivación: aproximación del péndulo simple]
    Sea $\theta''=-\frac{g}{l}\sin(\theta) = F$, aproximamos a través de $\hat{F} = -\frac{g}{l}$, con $|\theta| << 1$ y $\sin(\theta) \approx \theta$. Nos preguntamos si la solución de cada ecuación se parecen o no ya que resolver $\hat{F}$ es más sencillo que resolver $F$.
\end{eg}

La base del desarrollo formal de esta sección se basará en el siguiente lema.
\begin{lm}[de Gronwall]
    Sean $u,\ f\ g: [a, b] \to \R$ continuas, y $g \geq 0$. Supongamos que:
    $$
        u(t) \leq f(t) + \int_a^t g(s) u(s) \d s
    $$
    Entonces:
    $$
        u(t) \leq f(t) + \int_a^t f(s) g(s) e^{\int_s^t g(u) \d u} \d s
    $$
\end{lm}
\begin{proof}
    Sea $z(t) = \int_a^t g(s)\cdot u(s) \d s$, como $g$ y $u$ son continuas, $z'(t) = g(t)\cdot u(t)$, entonces por hipótesis:
    $$
        z'(t) \leq g(t) \cdot (f(t) + z(t)) = g\cdot f + g\cdot z \text{ ($g \geq 0$)}
    $$
    Sea $G(t)$ una primitiva de $g(t)$, es decir, $G'(t) = g(t)$ (por ejemplo, $G(t) = \int_a^t g(u) \d u$), entonces:
    \begin{gather*}
        z'(t) - g(t) \cdot z(t) \leq f(t) g(t) \\
        e^{-G(t)} (z'(t) - g(t) \cdot z(t)) \leq f(t) g(t) e^{-G(t)}\\
        \Dd{t} (e^{-G(t)} z(t) ) \leq f(t) g(t) e^{-G(t)} \implies \text{ (integrando entre $a$ y $t$) }\\
        \int_a^t (e^{-G}z)' \d s \leq \int_a^t f(s)g(s)e^{-G(s)} \d s
    \end{gather*}
    Desarrollando la primera parte de la desigualdad:
    $$
    \left. (e^{-G(s)}z(s)) \right|_{s=a}^{s=t} = e^{-G(t)}z(t) - e^{-G(a)}z(a) = \text{ (como $z(a) = 0$ ) } e^{-G(t)}z(t)
    $$
    Es decir:
    \begin{align*}
        e^{-G(t)}z(t) &\leq \int_{a}^b f(s) g(s) e^{-G(s)} \d s \implies\\
        z(t) &\leq e^{G(t)} \int_a^t f(s) g(s) e^{-G(s)} \d s\\
        &= \int_a^t f(s) g(s) e^{G(t) - G(s)} \d s
    \end{align*}
    y como $G(t) - G(s) = \int_s^t G'(u) \d u = \int_s^t g(u) \d u$, es decir, $z(t) = \int_a^t f(s) g(s) e^{\int_s^t g(u) d(u)}$ basta observar que:
    \begin{align*}
        u(t) &\leq f(t) + \int_a^t g(s) u(s) \d s = f(t) + z(t)\\
             &\leq f(t) + \int_a^t f(s) g(s) e^{\int_s^t g(u) d(u)}
    \end{align*}
\end{proof}
\begin{obs}
    Vamos a ver unos casos particulares:
    \begin{enumerate}
        \item $f = M$ constante.
        $$
            u(t) \leq M e^{\int_a^t g(s)} \d s
        $$
        \item $f = M$, $g = L$ constantes:
        $$
            u(t) \leq M e^{L(t-a)}
        $$
    \end{enumerate}
\end{obs}
\begin{pro}[Diferencia de soluciones de una ecuación. Cotas.]\label{pro:dif-sol-cot}
    Sea $\Omega$ abierto en $\R^d$, $F:[a, b] \times \Omega \to \R^d$ continua y tal que:
    $$
            |F(t, \xi) - F(t, \eta)| \leq  L |\xi - \eta| \text{ con } \xi, \eta \in \Omega,\ t\in[a, b],\ L\in \R,\ L > 0.
    $$
    Sean $X_1, X_2 :[a, b] \to \Omega$ y sean $\varepsilon_1, \varepsilon_2$ tal que:
    $$
        |X_j'(t) - F(t, X_j(t))| \leq \varepsilon_j \implies \ninf{X'_j - F(t, X_j)} \leq \varepsilon_j
    $$
    Entonces:
    $$
        |X_1(t) - X_2(t) | \leq |X_1(a) - X_2(a)| e^{L(t-a)} + (\varepsilon_1 + \varepsilon_2) \frac{e^{L(t-a)}-1}{L}
    $$
\end{pro}
\begin{proof}
    Dividiremos la prueba en partes:
    \begin{enumerate}
        \item Sea $u(t) = |X_1(t) - X_2(t)|$, con:
        \begin{gather*}
            X_1(t) = X_1(a) + \int_a^t X_1'(s) \d s\\
            X_2(t) = X_2(a) + \int_a^t X_2'(s) \d s
        \end{gather*}
        Como el valor absoluto de la integral es menor igual que la integral del valor absoluto, restando llegamos a que:
        $$
            u(t) \leq |X_1(a) - X_2(a)| + \int_a^t |X_1'(s) - X_2'(s)| \d s
        $$
        \item
        \begin{align*}
            |X_1' - X_2'| &\leq |X_1' - F(s, X_1)| + |X_2' - F(s, X_2)| + |F(s, X_1) - F(s, X_2)|\\
            &\leq \varepsilon_1 + \varepsilon_2 + L|X_1 - X_2|
        \end{align*}
        \item Entonces:
        \begin{align*}
            u(t) &\leq |X_1(a) - X_2(a)| + \int_a^t (\varepsilon_1 + \varepsilon_2 + L u(s) ) \d s
                 &\leq |X_1(a) - X_2(a)| + (\varepsilon_1 + \varepsilon_2)(t-a) + \int_a^t L u(s) \d s
        \end{align*}
        Entonces $u(t) \leq f(t) + \int_a^t g(s) u(s) \d s$.
        \item Aplicando en $LG$:
        $$
            u(t) \leq f(t) + \int_a^t L f(s) e^{L(t-s)} \d s
        $$
        e integrando por partes llegamos a la estimación del enunciado.
    \end{enumerate}
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Clase del 21/03
\subsection{Algunas consecuencias de la diferencia acotada de soluciones.}

Vamos a ver consecuencias de la proposición \ref{pro:dif-sol-cot}.\\
 Es claro que si $\varepsilon_1 = \varepsilon_2 = 0$ (es decir, son soluciones) y $X_1(a) = X_2(a)$ entonces $X_1 = X_2$ en $[a, b]$ (por unicidad).
\begin{pro}[Dependencia continua de condiciones iniciales]
    Sea $F$ que cumple las hipótesis de unicidad y existencia local. Sea $X(t, \xi)$ la solución (local) a:
    $$
        \begin{cases}
            X'(t) = F(t, X(t)) \text{ para } (\xi \in \Omega)\\
            X(a) = \xi
        \end{cases}
    $$
    Supongamos que $\xi_0 \in \Omega$. Entonces:
    \begin{enumerate}
        \item Existen $r>0,\ \delta > 0$ tal que $\exists X(t, \xi)$ tal que:
            $$
                X(t, \xi) : [a,\ a+\delta ] \times \bar{B}(\xi_0, \sfrac{r}{2}) \to \bar{B}(\xi_0, r) \text{ es solución}
            $$
        \item
        $$
            |X(t, \xi) - X(t, \eta)| \leq |\xi - \eta| e^{L(t-a)} \text{ con } \eta,\xi \in \bar{B}(\xi_0, \sfrac{r}{2}),\ a\leq t\leq t+\delta
        $$
        \item Más generalmente, si $M$ es una cota superior de $|F(t, \xi)|$ con $(t,\xi) \in [a,b]\times\bar{B}(\xi_0, r)$ y $F$ es continua entonces:
        $$
            |X(t, \xi) - X(\bar{t}, \eta)| \leq M |t - \bar{t}| + |\xi - \eta| e^{L(\min(t,\bar{t})-a)}
        $$
        En particular, $X$ es continua en ambas variables y $C^1$ en $t$.
    \end{enumerate}
\end{pro}
\begin{proof}
    Demostraremos cada apartado:
    \begin{enumerate}
        \item Directo por el teorema de existencia y unicidad local.
        \item Por la proposición \ref{pro:dif-sol-cot} con $\varepsilon_1 = \varepsilon_2 = 0$ y $\xi = X_1(a)$, $\eta = X_2(a)$, entonces:
        $$
            X_1(t) = X(t, \xi)\ \ \ X_2(t) = X(t, \eta)
        $$
        \item Supongamos sin pérdida de generalidad que $t < \bar{t}$, entonces:
        $$
            |X(t, \xi) - X(\bar{t}, \eta)| \leq |X(t, \xi) - X(t, \eta)| + |X(t,\eta) - X(\bar{t}, \eta)|
        $$
        Donde
        $$
        |X(t, \xi) - X(t, \eta)| < |\xi - \eta| e^{L(t-a)}
        $$
        y además:
        $$
            |X(t,\eta) - X(\bar{t}, \eta)| = |\int_t^{\bar{t}} X'(s, \eta) \d s| \leq |\int_t^{\bar{t}} F(t, X(s, \eta)) \d s | \leq \int_t^{\bar{t}} |F(s, X(s, \eta)) \d s | \leq M(\bar{t} - t) = M|(t-\bar{t})|
        $$
        Y uniendo las dos desigualdades hallamos la desigualdad que queríamos demostrar.
    \end{enumerate}
\end{proof}
\begin{cor}
    Si $F = F(t, \xi, \lambda)$ es Lipschitz en $(\xi, \lambda)$, pasando al problema:
    $$
        \mx{X\\\lambda}' = \mx{X'\\ 0} = \mx{F\\0}
    $$
    y usando el resultado anterior, vemos que la solución de $X(t, \xi, \lambda)$ de:
    $$
        \begin{cases}
            X' = F(t, X, \lambda)\\
            X(a) = \xi
        \end{cases}
    $$
    es continua en todas las variables.
\end{cor}

\begin{pro}[Cota de error en aproximación de soluciones]
    Sea $\hat{F} : [a, b] \times \Omega \to \R^d$ Lipschitz.\\
    Sea $\hat{X}(t)$ la solución de:
    $$
        \begin{cases}
            \hat{X}'(t) = \hat{F}(t, \hat{X}(t))\\
            \hat{X}(a)=\xi_a
        \end{cases}
    $$
    y $X$ la de:
    $$
    \begin{cases}
        X'(t) = F(t, X(t))\\
        X(a)=\xi_a
    \end{cases}
    $$
    Supongamos que
    $$
        |\hat{F} (t, \hat{X}(t)) - F(t, \hat{X}(t))| \leq \varepsilon
    $$
    Entonces:
    $$
        |X(t) - \hat{X}(t)| \leq \varepsilon \cdot \frac{e^{L(t-a)}-1}{L}
    $$
     donde $L$ es la constante de Lipschitz de $F$
\end{pro}
\begin{proof}
    Decimos que $\hat{X}$ es casi solución para $F$, es decir:
    $$
        |\hat{X}'(t) - F(t, \hat{X}(t))| \leq \varepsilon
    $$
    ya que:
    \begin{gather*}
        \hat{X}'(t) = \hat{F}(t, \hat{X}(t))\\
        |\hat{X}'(t) - F(t, \hat{X}(t))| = |\hat{F} (t, \hat{X}(t)) - F(t, \hat{X}(t))| \leq \varepsilon \text{ por hipótesis}
    \end{gather*}
    Además, con:
    $$
        X_1 = X,\ X_2 = \hat{X},\ |X_1(a) - X_2(a)| = 0
    $$
    es decir,
    $$
        \varepsilon_1 = 0,\  \varepsilon_2 = \varepsilon, \text{ y } X_1(a) = X_2(a) = \xi_a
    $$
    Entonces:
    $$
        |X(t) - \hat{X}(t)| = |X_1(t) - X_2(t)| \leq \varepsilon \frac{e^{L(t-a)}-1}{L}
    $$
\end{proof}
\begin{eg}[Péndulo simple. Aproximación de soluciones]
    Sea la EDO $\theta'' = -\frac{g}{l}\sin(\theta)$ donde $l$ es la longitud del péndulo. Transformamos la ecuación a su forma matricial:
    $$
        \mx{\theta\\v}' = \mx{v\\-\frac{g}{l}\sin{\theta}} = F(\theta, v)
    $$
    Observamos que para $\theta$ pequeño, $\sin(\theta) \approx \theta$. Entonces construimos nuestro problema aproximado:
    $$
        \hat{\mathcal{P}} \equiv \theta''=-\frac{g}{l} \theta \leadsto \mx{\theta\\v}'=\mx{v\\-\frac{g}{l}\theta} = \hat{F}
    $$
    Este problema es más sencillo de resolver y hallamos que la solución a $\hat{\mathcal{P}}$ es:
    \begin{gather*}
        \hat{\theta}(t)=\theta_0\cos(\omega t) \text{ con }\omega = \sqrt{\frac{g}{l}}
    \end{gather*}
    Es decir, nuestra solución $\hat{X}$ es:
    $$
        \hat{X} = \mx{\hat{\theta}(t)\\ \hat{v}(t)} = \mx{\theta_0\cos(\omega t)\\-\omega \theta_0 \sin(\omega t)}
    $$
    Donde además, $|\theta(t)| \leq |\theta_0|$. Nos gustaría comparar entonces $F$ y $\hat{F}$ en la solución $\hat{X}$.
    \begin{gather*}
        |\hat{F}(\hat{\theta}(t), \hat{v}(t)) - F(\hat{\theta}(t), \hat{v}(t))| = \mx{0\\-\frac{g}{l}(\hat{\theta}(t) - \sin(\hat{\theta}(t)))}=\\
        = \frac{g}{l} |\hat{\theta}(t) - \sin(\hat{\theta}(t))| \leq^{(*)} \frac{g}{l}\frac{|\hat{\theta}(t)|^3}{6}
    \end{gather*}
    \textit{(*)} este último paso se debe al lema \ref{lm:cota-seno}. Como $\theta - \sin\theta$ es impar, $|\theta-\sin\theta| = |\theta| - \sin(|\theta|) \leq \frac{|\theta|^3}{6}$\\\\
    Entonces:
    $$
        |\hat{F}(\hat{\theta}(t), \hat{v}(t)) - F(\hat{\theta}(t), \hat{v}(t))| \leq \frac{g}{l} \cdot \frac{|\hat{\theta}|^3}{6} \leq \frac{g}{l} \cdot \frac{|\theta_0|^3}{6}
    $$
    Como consecuencia:
    $$
        |X(t) - \hat{X}(t)| \leq |\theta(t) - \hat{\theta(t)}| + |\theta'(t) - \hat{\theta}'(t)| \leq \frac{g}{l}\frac{|\theta_0|^3}{6} \cdot \frac{e^{Lt} - 1}{L}
    $$
    Nos faltaría encontrar el valor de $L$. Para ello desarrollamos:
    $$
        F = \mx{v\\-\frac{g}{l}\sin(\theta)},\ \frac{\partial F}{v} = \mx{1\\0},\ \frac{\partial F}{\theta} = \mx{0\\-\frac{g}{l}\cos(\theta)}
    $$
    Y de aquí, si $\left|\frac{\partial F_i}{\partial x_j}\right| \leq L$ en $[a, b]\times \Omega$ con $\Omega$ convexo entonces:
    $|F(t, \xi) - F(t, \eta)| \leq L |\xi - \eta|$ con $\xi,\ \eta \in \Omega$, y de aquí obtenemos:
    $$
        L = \max\left(1, \frac{g}{l}\right)
    $$

\end{eg}

\begin{lm}[Cota del seno de un ángulo positivo]\label{lm:cota-seno}
    Si $\theta \geq 0$, entonces:
    $$
        \theta - \frac{\theta^3}{3!} \leq \sin(\theta) \leq \theta
    $$
\end{lm}
\begin{proof}
    Se deja al lector. Se basa en la serie de potencias del seno.
\end{proof}
\subsection{Diferenciabilidad con respecto a par\'ametros y datos iniciales}
Vamos a considerar $F(t, X, \lambda) : (a, b) \times \Omega \times \Delta \to \R^d$, con $\Omega$ un abierto de $\R^d$, $\Delta$ abierto en $\R^m$. Además, denominaremos $D_xF$ a la matriz de las derivadas parciales, donde cada elemento $(i,j)$ es de la forma:
$
    \frac{\partial F_i}{\partial x_j}
$. De la misma forma $D_\lambda F$ es la matriz de derivadas parciales, donde cada elemento $(i, k)$ es de la forma:
$
    \frac{\partial F_i}{\partial \lambda_k}
$.
Además, sea $(t_0, \xi_0, \lambda_0)\in V$, supongamos que $F$ es continua y las derivadas parciales son continuas en $V$.
\begin{thm}[Diferenciabilidad en ecuaciones de orden 1]
    Si
        $\exists \delta > 0 : \text{ si } |\xi - \xi_0| < \delta,\text{ y } |\lambda - \lambda_0| < \delta$ entonces existe una solución (que es única) del problema:
        $$  \mathcal{P}_{\xi,\lambda}
            \begin{cases}
                    X'(t) = F(t, X(t), \lambda)\\
                    X(t_0) = \xi
            \end{cases} \text{ en } [t_0,\ t_0+\delta]
        $$
        Esa solución $X(t, \xi, \lambda)$ es $C^1$ y sus derivadas con respecto a $\lambda_k$ o $\xi_j$ satisfacen una EDO.\\\\
        Por ejemplo, si $Y(t) = \frac{\partial X}{\partial \lambda_k}(t, \xi, \lambda)$, entonces $Y$ satisface:
        $$
            Y'(t) = D_xF(t, X(t, \xi, \lambda), \lambda) \cdot Y(t) + \frac{\partial F}{\partial \lambda_k}(t, X(t, \xi, \lambda), \lambda)
        $$
        Donde si :
        $$
            A(t) = D_xF(t, X(t, \xi, \lambda), \lambda),\text{ y } B(t) = \frac{\partial F}{\partial \lambda_k}(t, X(t, \xi, \lambda), \lambda)
        $$
        entonces reescribimos:
        $$
            Y'(t) = A(t)Y + B(t)
        $$
        Además, en términos de matrices si $M(t) = D_\lambda X(t, \xi, \lambda)$ entonces:
        $$
            \begin{cases}
                M'(t) = A(t) M(t) + D_{\lambda}F(t, X(t, \xi, \lambda), \lambda)\\
                M(t_0) = 0
            \end{cases}
        $$
        y si $N(t) = DX$ entonces:
        $$
            \begin{cases}
                N' = A(t) N\\
                N(0) = I
            \end{cases}
        $$
        %%TODO: ENTENDER EL EJEMPLO Y REDACTARLO MEJOR

\end{thm}
