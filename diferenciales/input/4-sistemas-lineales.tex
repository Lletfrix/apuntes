% !TeX root = ../ecuaciones-diferenciales.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Clase del 25/02
\chapter{Sistemas lineales de orden I}
\section{Introducci\'{o}n a sistemas}
En esta sección vamos a ver como resolver un sistema de ecuaciones diferenciales de orden I. Estos sistemas son del tipo:
$$
\begin{cases}
    x' = f(t, x, y)\\
    y' = g(t, x, y)
\end{cases}
$$
y habitualmente los escribiremos:
$$
X' = F(t, X)
$$ Donde:

\begin{gather*}
X(t) =
\left(
\begin{matrix}
    x(t)\\
    y(t)
\end{matrix}
\right)\\
F(t, X) =
\left(
\begin{matrix}
    f(t, x, y)\\
    g(t, x, y)
\end{matrix}
\right)
\end{gather*}

\begin{eg}[Sistema lineal de orden I a partir de una ecuación de orden II]
    Sea $x'' + \mu\cdot (1-x^2)x' + x = 0$, vamos a estudiarla como sistema.\\
    Consideramos y = x'. Tenemos:
    \begin{gather*}
        X = \left(\begin{matrix}
            x\\
            y
    \end{matrix}
    \right) \\
    x' = y = f(t, x, y) \\
    y' = (x'') = -x -\mu (1-x^2)y = g(t, x, y)
    \end{gather*}
    entonces obtenemos el sistema:
    $$\left(
        \begin{matrix}
            x\\
            y
        \end{matrix}\right)' =
        \left(
        \begin{matrix}
            y \\
            -x -\mu (1-x^2)y
        \end{matrix}\right)
    $$
\end{eg}

Como hemos visto, podemos transformar una ecuación diferencial de orden II en un sistema de ecuaciones de orden I. De hecho, podremos transformar cualquier EDO en un sistema de orden I.\\
\begin{obs}
    Cualquier sistema se puede convertir en un sistema autónomo, es decir, no depende de la variable $t$.\\
    Sea nuestro sistema: $X' = F(t, X)$ con $X: (\alpha, \beta) \to \R^n$, $F:(\alpha, \beta) \times \Omega \subset \R^n \to \R^n$. Podemos tomar:
    $$
        Y = \left(\begin{matrix}
            t\\
            X
    \end{matrix}\right),\ Y' = \left(\begin{matrix}
        1\\
        F(t, X)
    \end{matrix}\right) = \left(\begin{matrix}
        1\\
        F(Y)
    \end{matrix}\right) = G(Y)
    $$
    Y de esta forma hemos transformado nuestro sistema $X' = F(t, X)$ en uno autónomo $Y' = G(Y)$.
\end{obs}
\section{Unicidad, existencia y estructura de soluciones.}
Consideraremos a partir de ahora el sistema:
$$
    X' = \mathbb{A}(t) X + B(t)
$$
con $\mathbb{A} \in \R^{n\times n}$, y $X, B \in \R^{n}$
\begin{thm}[Teorema de existencia, unicidad y estructura]\label{thm:uni-exi-estr}
    Sean $\mathbb{A} : [\alpha, \beta] \to \R^{n\times n}$ y $B : [\alpha, \beta] \to \R^n$ ambas continuas. Y sea $t_0 \in [\alpha, \beta]$, $X_0 \in\R^n$.Entonces:\\
    \begin{enumerate}
        \item Existe una solución $X(t)$ del PVI:
        $$
        \begin{cases}
            X' = \mathbb{A}(t) X + B(t)\\
            X(t_0) = X_0
        \end{cases}
        $$
        es decir, $\exists X : [\alpha, \beta] \to \R^n$ que cumple el PVI y esa solución es única.

        \item Para la ecuación homogénea asociada $(\mathcal{EDL}_h) \equiv X' = \mathbb{A}(t) \cdot X$:
        \begin{itemize}
            \item Existen $X_1(t) \ldots X_n(t)$ linealmente independientes, que son soluciones de $(\mathcal{EDL}_h)$ en $t \in [\alpha, \beta]$.
            \item Si $X_1(t) \ldots X_n(t)$ son soluciones linealmente independientes de $(\mathcal{EDL}_h)$, entonces:
            $$
            X(t) \text{ es solución de } (\mathcal{EDL}_h) \text{ en } [\alpha, \beta] \iff \exists c_1, c_2, \ldots c_n \in \R : X(t) = c_1X_1(t) + \ldots + c_nX_n(t)
            $$
        \end{itemize}
        \item Si $X_p(t)$ es una solución de $(\mathcal{EDL})$, entonces $X(t)$ es solución de $(\mathcal{EDL})$ (en $[\alpha, \beta]$) $\iff$
        $$\exists c_1, \ldots, c_n \in \R : X = X_p + \sum_{j=1}^n c_jX_j$$
        donde $\sum_{j=1}^n c_jX_j$ es la solución general de la homogénea.
    \end{enumerate}
\end{thm}

\begin{proof}
    Veamos la demostraciones de cada punto:\\
    \begin{enumerate}
        \item Lo veremos más adelante.
        \item Elegimos $t_0 \in [\alpha, \beta]$. Sea $X_j(t)$ \textbf{la} solución del PVI con
        $$
            X_j(t_0) = \left[\begin{matrix}
                0\\
                \vdots\\
                0\\
                1\\
                0\\
                \vdots\\
                0
        \end{matrix}\right] = e_j \text{ vector j-ésimo de la base canónica.}
        $$
        Afirmamos que son linealmente independientes pues
        $$
            \sum c_j X_j(t) = 0\ \forall t \in [\alpha, \beta] \implies \sum c_jX_j(t_0) = 0 \implies c_i = 0 \forall i
        $$
        Entonces:\\
        $X_1, \ldots, X_n$ son soluciones de $(\mathcal{EDL}_h)$ linealmente independientes. La implicación $(\impliedby)$ está demostrada, pues si $X = \sum c_j X_j$ entonces $X$ es solución. Vamos a ver la implicación $(\implies)$, que la resolveremos por reducción al absurdo.\\\\
        Elegimos $t_0 \in [\alpha, \beta]$. Entonces $X_1(t_0) + \ldots + X_n(t_0)$ son linealmente independientes. Si no lo fueran, existirían $c_1, \ldots, c_n : \exists c_i \neq 0$ y que $c_1X_1(t_0) + \ldots c_n X_n(t_0) = 0$. Y esto haría que:
        $$
            c_1X_1(t) + \ldots c_nX_n(t)
        $$ resuelve:
        $$
        \begin{cases}
            X' = \A(t)X\\
            X(t_0) = 0
        \end{cases} \implies X(t) = 0 \text{ también es solución.}
        $$
        Por unicidad es una contradicción.\\\\
        Además, si $X(t)$ es solución de $(\mathcal{EDL}_h)$, entonces acabamos de ver que $\exists c_1, \ldots, c_n$ tal que $X(t_0) = \sum c_jX_j(t_0)$ (donde $X_j$ forman una base de $\R^n$). Por unicidad:
        $$
            X(t) = \sum_{j=1}^n c_jX_j(t)\ \forall t\in[\alpha, \beta]
        $$
        \item Vamos a ver ques si tenemos dos soluciones que resuelven el sistema, entonces la resta es la solución de la homogénea. Es decir, si $X(t)$ resuelve $X' = \A(t) + B(t)$, sea:
        $$
            Y(t) \equiv X(t) - X_p(t)
        $$
        entonces:
        $$
            Y' = X' - X_p' = (\A(t)X + B(t)) - (\A(t)X_p + B(t)) = \A(t)X - \A(t)X_p = \A(t)Y \implies
            Y = c_1X_1 + \ldots c_nX_n
        $$
    \end{enumerate}
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Clase del 26/02
\begin{obs}
    El apartado 2 nos dice que el conjunto de soluciones de la $(\mathcal{EDL}_h)$ es un espacio vectorial de dimensión n.
\end{obs}
Durante la demostración del apartado 2 vimos aproximadamente el siguiente lema:
\begin{pro}\label{pro:lema-wronskiano}
    Sean $X_1(t) + X_2(t) + \ldots + X_n(t)$ solucion de la $(\mathcal{EDL}_h)$ son equivalentes:\\
    \begin{enumerate}
        \item $X_1, \ldots, X_n$ son linealmente independientes como funciones, es decir,
        $$\sum c_j X_j (t) = 0 \text{ con } t \in [\alpha, \beta] \implies c_j = 0 \ \forall j$$
        \item
        $$
            \exists t_0 \in [\alpha, \beta] : X_1(t_0), \ldots, X_n(t_0) \text{ son vectores linealmente independientes.}
        $$
        \item
        $$
            \forall t \in [\alpha, \beta], \text{ entonces } X_1(t), \ldots, X_n(t) \text{ son vectores linealmente independientes.}
        $$
    \end{enumerate}
\end{pro}
\begin{proof}
    En la demostración del apartado 2 del teorema \ref{thm:uni-exi-estr} vimos que $1 \iff 2$ y $3$ se sigue sin dificultad.
\end{proof}
\begin{obs}
    \begin{itemize}
    \item El enunciado del lema sigue siendo cierto si hay $m \leq n$ soluciones con n la dimensión del espacio de soluciones.
    \item $X_1(t_0) \ldots X_n(t_0)$ son linealmente independientes $\iff \det \mx{\uparrow && \uparrow\\ X_1(t_0) & \cdots & X_n(t_0) \\ \downarrow && \downarrow} \neq 0$
    \end{itemize}
\end{obs}
\begin{dfn}[Wronskiano]\label{dfn:wronskiano}
    El \textbf{wronskiano} de $X_1,  \ldots, X_n$ es la función:
    $$
        W(t) = \det \mx{\uparrow && \uparrow\\ X_1(t_0) & \cdots & X_n(t_0) \\ \downarrow && \downarrow}
    $$
\end{dfn}
Tras ver esta definición del wronskiano, lo que dice parte del lema \ref{pro:lema-wronskiano} es que:
$$
    W(t) \neq 0\ \forall t \in [\alpha, \beta] \iff \exists t_0 \in [\alpha, \beta] : W(t_0) \neq 0
$$
Más adelante veremos que $W(t)$ satisface la ecuación:
\begin{gather*}
    W'(t) = a(t) W(t)\ :\ t\in [\alpha, \beta]\\
    a(t) = Traza(\A(t))
\end{gather*}
De ahí $W(t) = W(t_0) e^{\int_{t_0}^{t} a(s) ds}$ y se ve que $W(t) = 0$ ó $W(t) \neq 0\ \forall t\in [\alpha, \beta]$

\section{Sistema lineal con coeficientes constantes.}

Vamos a considerar sistemas lineales de ecuaciones diferenciales cuyos coeficientes no son una variable del problema.

\begin{eg}[Sistemas lineales con coeficientes constantes - Idea básica]
    Vamos a considerar:
    $$
        X'(t) = \A X(t),\text{ con } \A_{n\times n} \text{ constante.}
    $$
    Procediendo de forma similar a como lo hacíamos en el caso de ecuaciones lineales de segundo orden, vamos a probar $X(t) = e^{\lambda t} V : V\in\R^{n},\ V \text{ constante}$ como solución.
    Vamos a ver como hallamos $\lambda$ y $V$.\\\\
    $$
        X'(t) = \lambda e^{\lambda t} V \equiv \A X(t) = \A e^{\lambda t} V = e^{\lambda t} \A V
    $$
    por tanto:
    $$
        X' = \A X \iff e^{\lambda t}(\A V - \lambda V) = 0 \iff \A V = \lambda V
    $$
    es decir:
    $$
        V \text{ es autovector de } \A \text{ con autovalor } \lambda
    $$
\end{eg}

\begin{eg}[Sistema lineal con matriz diagonalizable]
    Sea
    $$
        X' = \mx{2 & 0 & -1 \\ 1 & 1 & -1 \\ 0 & 0 & 1} X
    $$
    entonces es fácil ver que los autovalores de $\A$ son $1,\ 1,\ 2$. Con sus correspondientes autovectores:
    $$
        V_1 = \mx{1 \\ 2 \\ 1},\ V_2 = \mx{1\\0\\1},\ V_3 = \mx{1\\1\\0}
    $$
    Con lo que tenemos tres soluciones:
    \begin{gather*}
        X_1(t) = e^t V_1\\
        X_2(t) = e^t V_2\\
        X_3(t) = e^{2t} V_3\\
    \end{gather*}
    que son base del espacio de soluciones de $X' = \A X$\\\\
    Hemos podido hacer esto por que la matriz es diagonalizable, es decir
    $$
        \exists \B : \B^{-1}\A\B = \mx{1&0&0\\0&1&0\\0&0&2}
    $$
    donde la diagonal son los autovalores.
\end{eg}
\begin{eg}[Sistema lineal con matriz no diagonalizable]
    Sea
    $$
        X' = \mx{2 & 1\\0&2} X = \A X
    $$
    De aquí hallamos que los autovalores son $2, 2$.
    Entonces:
    $$
        \mx{2 & 1\\0&2} \mx{a \\ b} = 2 \mx{a \\ b}
    $$
    donde hallamos:
    \begin{gather}
        2a+b=2a\\
        2b=2b\\
        b=0
    \end{gather}
    Entonces hemos hallado un autovector:
    $$
        \mx{1\\0} \implies X_1(t) = e^{2t}\mx{1\\0}
    $$
    Para buscar otra solución intentamos:
    $$X(t) = e^{2t} (C + tD)$$
    donde $C$, $D \in \R^2$.
    Entonces:
    $$
        X' = e^{2t}(2C + D + 2tD) \equiv \mx{2 & 1\\0&2} X = e^{2t}(\A C + t \A D)
    $$
    es decir, se necesita:
    \begin{gather*}
        \A D = 2 D \implies D = \mx{1\\0} \text{ por ejemplo.}\\
        \A C = 2C + D
    \end{gather*}
    es decir, $(\A - 2I) C = D$ con $I$ la identidad. Tenemos:
    $$
    \mx{0 & 1\\0&0}\mx{a\\b} = \mx{1\\0} \implies b=1, \text{ vale cualquier valor para a.}
    $$
    entonces:
    $$
        \mx{a\\1} = a\mx{1\\0} + \mx{0\\1} \implies a = 0
    $$
    Por tanto podemos hallar $C = \mx{0\\1}$ y $D$ era $\mx{1\\0}$. Por tanto nuestras dos soluciones son:
    \begin{gather*}
        X_1(t) = e^{2t}\mx{1\\0}\\
        X_2(t) = e^{2t}\left(\mx{0\\1} + t \mx{1\\0}\right)
    \end{gather*}
\end{eg}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Clase del 27/02
\section{Exponencial de una matriz}
Para esto, vamos a construir la exponencial de una matriz.\\\\
Sea $\A \in \C^{d\times d}$. Vamos a intentar construir la exponencial de la misma forma que hace Taylor cuando trabajamos con escalares.
\begin{pro}
    La serie:
    $$
        \sum_{n=0}^\infty \frac{\A^n}{n!}
    $$
    converge (sea cual sea $\A$)
\end{pro}
\begin{proof}
    Empezamos viendo ciertos aspectos.\\
    \begin{enumerate}
        \item Definimos:
            $$
            ||\A|| = \max_{x\neq0} \frac{||Ax||_2}{||x||_2} = \max_{||y||_2=1}||Ay||_2
            $$
            con $x, y \in \C^d$ y $||z||_2 = \sqrt{|z_1|^2+|z_2|^2+\ldots+|z_d|^2}$ la norma euclídea.\\
            Se puede comprobar que lo que acabamos de definir es una norma y tiene una propiedad importante:
            $$
                ||\A \B|| \leq ||\A||||\B|| \text{ y en particular } ||\A^n|| \leq ||\A||^n
            $$
        \item Es fácil ver que:
            $$
                \left|\left|\sum_{n=0}^{\infty} \frac{\A^n}{n!}\right|\right| \leq \sum_{n=0}^{\infty} \frac{||\A||^n}{n!} \text{ y esta última converge.}
            $$
    \end{enumerate}
    Entonces, $\sum_{n=0}^{\infty} \frac{\A^n}{n!}$ converge y:
    $$
        \left|\left|\sum_{n=0}^{\infty} \frac{\A^n}{n!}\right|\right| \leq \sum_{n=0}^{\infty} \frac{||\A||^n}{n!}
    $$
\end{proof}
\begin{dfn}[Exponencial de una matriz]
    Definimos la exponencial de una matriz $\A$ como:
    $$
        e^{\A} = \exp(\A) = \sum_{n=0}^{\infty} \frac{\A^n}{n!}
    $$
\end{dfn}
Vamos a ver ciertas propiedades:\\
\begin{pro}[Exponencial de una matriz - propiedades]
\begin{enumerate}
    \item Si $\A$ y $\B$ conmutan $(\A\B = \B\A)$ entonces:
    $$
        e^{\A+\B} = e^\A e^{\B}
    $$
    \item Si $\A$ es diagonal ($\diag(\lambda_1, \lambda_2,\ldots)$), entonces:
    $$
        e^\A = \diag(e^{\lambda_1}, e^{\lambda_2}, \ldots)
    $$
    \item Si $\A = \S \B \S^{-1}$, entonces:
    $$
        e^{\A} = \S e^{\B} \S^{-1}
    $$
    \item Si $\B$ es una matriz de bloques diagonales:
    $$
        \B = \mx{\B_1 & 0 & 0 & \cdots \\
            0  & \B_2 & 0 & \cdots\\
            0 & 0 & \B_3 & \cdots\\
            \vdots & \vdots & \vdots & \ddots} \text{ con } \B_i \in \C^{m\times n}
    $$
    entonces:
    $$
        \B^n = \diag_b(\B_1^n, \B_2^n, \ldots, \B_m^n)
    $$
    y por tanto:
    $$
        e^{\B} = \diag_b(e^{\B_1},e^{\B_2},\ldots,e^{\B_n})
    $$
    \item
        $$
            e^{(t+s)\A} = e^{t\A}e^{s\A}
        $$
    \item
        $$
            e^{t\A} \text{ es invertible y su inversa es } e^{-t\A}
        $$
\end{enumerate}
\end{pro}

\begin{proof}
    Vamos a demostrar cada propiedad:\\
    \begin{enumerate}
        \item $\A, \B$ conmutan entonces:
        $$
            (\A + \B)^m = \sum_{k=0}^m
            \left(\begin{matrix}
                m\\
                k
            \end{matrix} \right)
            \A^k \B^{m-k}
        $$
        Por tanto:
        $$
            e^\A e^\B = \sum_{k=0}^{\infty}\frac{A^k}{k!} \cdot  \sum_{k=0}^{\infty}\frac{A^k}{k!} = (\cdots) = \sum_m^{\infty} \frac{1}{m!}(\A+\B)^m = e^{\A+\B}
        $$
        \item
        $$
            \mx{\lambda_1& &0\\
                &\ddots&\\
                0& &\lambda_d}^m =
                \mx{\lambda_1^m&&0\\
                    &\ddots&\\
                    0&&\lambda_d^m}
        $$
        \item
        \begin{gather*}
                \A = \S \B \S^{-1}\\
                \A^2 = \S \B \S^{-1}\S \B \S^{-1} = \S \B^2 \S^{-1}\\
                \A^m = \S \B^m \S^{-1} \implies \sum \frac{\A^m}{m!} = \S \frac{\B^m}{m!} \S^{-1}
        \end{gather*}
        \item
        $$
            \mx{\B_1 & 0 \\
                0 & \B_2} ^2 = \mx{\B_1^2 & 0 \\
                    0 & \B_2^2}
        $$
        \item $t\A = s \A$ conmutan
        \item Por la propiedad anterior:
        $$
            I = e^{0 \A} = e^{(t-t)A} = e^{t\A}e^{-t\A}
        $$
    \end{enumerate}
\end{proof}

\begin{pro}
    Sea $\A \in \C^{d\times d}$\\
    $$
        \Dd{t} e^{t\A} = \A e^{t\A} = e^{t\A} \A
    $$
\end{pro}
\begin{proof}
    A completar
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Clase del 28/02
\subsection{$e^{\A t}$ y la forma de Jordan}
Diremos que una matriz $\A$ está expresada en forma canónica de Jordan cuando se encuentra expresada de la forma:
$$
    \A = \S \J \S^{-1}
$$
Donde $\J$ es una matriz de bloques, de la forma:
$$
\J = \mx{\J_1 & 0 & 0 & \cdots \\
    0  & \J_2 & 0 & \cdots\\
    0 & 0 & \J_3 & \cdots\\
    \vdots & \vdots & \vdots & \ddots} \text{ con } \J_i \in \C^{m\times n}
$$
Vamos a ver la expresión en forma canónica de Jordan de $e^{\A t}$, con $\A =  \S \J \S^{-1}$.\\
\begin{enumerate}
    \item Como vimos en las propiedades de la exponencial matricial:
    $$
        e^{\A t} = \S e^{\J t} \S^{-1} = S \mx{t\J_1 & 0 & 0 & \cdots \\
            0  & t\J_2 & 0 & \cdots\\
            0 & 0 & t\J_3 & \cdots\\
            \vdots & \vdots & \vdots & \ddots} \S^{-1}
    $$
    \item Si el bloque $\B = \J_i$ es de la forma: $\B = \diag(\lambda, \lambda, \ldots, \lambda)$ entonces:
    $$
        e^{t\B} = \diag(e^{t\lambda},e^{t\lambda},\ldots, e^{t\lambda}) = e^{t\lambda} I
    $$
    \item Si $\B = \J_i$ es de la forma:
    $$
    \mx{\lambda & 1 & 0  &\cdots & 0\\
            0 & \lambda & 1  &\cdots & 0\\
        \vdots & \vdots & \vdots & \ddots &\vdots\\
        0 & 0 & 0 & \cdots & 1\\
        0 & 0 & 0 & \cdots & \lambda\\
    } \implies \B = \lambda I +\N
    $$ con
    $$
        \N = \mx{0 & 1 & 0  &\cdots & 0\\
                0 & 0 & 1  &\cdots & 0\\
            \vdots & \vdots & \vdots & \ddots &\vdots\\
            0 & 0 & 0 & \cdots & 1\\
            0 & 0 & 0 & \cdots & 0\\
        }
    $$
    y se puede comprobar que si $\N$ es $m\times m$ entonces
    $$
        \N^m = 0
    $$
    Por tanto, $$t\B = \lambda t I + t\N \implies e^{t\B} = e^{\lambda t I + t \N} = e^{\lambda t} I e^{t \N} =  e^{\lambda t} e^{t \N}$$
    y a partir del desarrollo de la exponencial:
    $$
        e^{t\N} = \sum_{k=0}^{m-1} \frac{(t\N)^k}{k!}
    $$
    ya que los términos con $k \geq m$ son nulos pues $\N^k : k\geq m$ se anula.\\
    Tenemos por tanto que:
    $$
        e^{t\B} = \mx{
                        e^{\lambda t} & t e^{\lambda t} & \frac{t^2}{2!}e^{\lambda t} & \cdots\\
                        0 & \ddots & t e^{\lambda t} & \frac{t^2}{2!}e^{\lambda t}\\
                        0  & 0 & \ddots  & t e^{\lambda t}\\
                        0 & 0 & 0 & e^{\lambda t}
                    }
    $$
    %%Incluir recordatorio de aparición de bloques??
\end{enumerate}

\begin{eg}[Matriz de Jordan dada $\A$]
    Sea $\A$ una matriz\\
    $$
        \A = \mx{
                    0 & -1 & 0 & 0\\
                    1 & -2 & 0 & 0\\
                    1 & -1 &-1 & 0\\
                    -8& 4 &  4 & 3
                } \implies P(\lambda) = \det(\A - \lambda I)
    $$
    Tras calcular, obtenemos los autovalores:
    \begin{gather*}
        -1 \text{ triple}\\
        3
    \end{gather*}
    y los autovectores: $\mx{0& 0& 0& -1}^T$ del autovalor $3$, y $\mx{1& 1& 0& 1}^T$, $\mx{1& 1& 1& 0}^T$ del autovalor $-1$. Es decir, necesitamos un autovalor generalizado para $-1$ pues solo tenemos $3$ autovectores.\\
    Para ello, tenemos que calcular $(\A - \lambda I)^m$ con $m > 1$ y lo más próximo al numero de autovectores que nos faltan, es decir $m=2$. Como necesitamos generalizar para el autovalor $\lambda = -1$, entonces calculamos:
    $$
        (\A + I) ^2 = \mx{
                            0 & 0 & 0 & 0\\
                            0 & 0 & 0 & 0\\
                            0 & 0 & 0 & 0\\
                            -32 & 16 & 16 & 16\\
                         }
    $$
    El autovector $v$ generalizado tendrá que cumplir:
    $$
        (\A+I)^2 v = 0 \text{ y } (\A+I)v \neq 0 \implies v = \mx{1 & 0 & 1 & 1}^T
    $$
    Con esto, hemos hallado $\J$ y $\S$:
    $$
        \J = \mx{
                    -1 & 0 & 0 & 0\\
                    0 & 3 & 0 & 0\\
                    0 & 0 & -1 & 1\\
                    0 & 0 & 0 & -1
                }\ \ \ \S = \mx{\uparrow &\uparrow &\uparrow &\uparrow \\v_{-1} & v_{3} & v_{1} & v_{gen}\\ \downarrow & \downarrow & \downarrow & \downarrow} = \mx{1 & 0 & 1 & 1\\
                                                                      1 & 0 & 1 & 0\\
                                                                      0 & 0 & 1 & 1\\
                                                                      1 & -1 & 0 & 1
                                                                        }
    $$
    y finalmente:
    $$
        e^{t\A} = \S e^{t \J} \S^{-1}
    $$
\end{eg}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Clase 04/03
\section{Matriz fundamental}
\begin{dfn}[Matriz fundamental]
    Llamamos \textbf{matriz fundamental} a :
   $$
    F(t) = \mx{\uparrow && \uparrow\\ X_1(t_0) & \cdots & X_n(t_0) \\ \downarrow && \downarrow}
   $$
   con $X_1, \ldots, X_n$ son soluciones linealmente independientes y cuyo determinante es el wronskiano, según vimos en la definición \ref{dfn:wronskiano}.
\end{dfn}
\begin{obs}
    Sea $(\mathcal{EL}_h) \equiv X' = \A X$ con $\A$ una matriz $d\times d$. Sea $\F$ la matriz fundamental.\\
    \begin{enumerate}
        \item La solución general de $(\mathcal{EL}_h)$ es $\F(t) \cdot C$ con:\label{obs:sol-gen-fund}
        $$
            C = \mx{c_1\\c_2\\\ldots\\c_n} = \sum_{j=1}^d c_jX_j(t)
        $$
        \item Esto vale para $X' = \A(t) X$. Por tanto, podemos afirmar que:
        $$
            \F'(t) = \A \F(t) \text{ con } \det(\F(t)) \neq 0\ \forall t\in \R
        $$
        es decir,
        $$
             \mx{\uparrow && \uparrow\\ X_1'(t_0) & \cdots & X_n'(t_0) \\ \downarrow && \downarrow} = \A  \mx{\uparrow && \uparrow\\ X_1(t_0) & \cdots & X_n(t_0) \\ \downarrow && \downarrow} =  \mx{\uparrow && \uparrow\\ \A X_1(t_0) & \cdots & \A X_n(t_0) \\ \downarrow && \downarrow}
        $$
    \end{enumerate}
\end{obs}

En general, vamos a buscar una $\F$ especial; la que cumple que $\F(0) = I$. Vamos a ver que esta $\F$ es de la forma:
$$
    \F(t) = e^{t\A}
$$
\begin{pro}[Matriz fundamental principal]
    Sea $X' = \A X$, afirmamos que $e^{t\A}$ es una matriz fundamental que además cumple que su valor en $t=0$ es $I_{n}$.
\end{pro}
\begin{proof}
    \begin{gather*}
        \Dd{t} e^{t\A} = \A e^{t\A}\\
        e^{0\A} = I ((e^{tA})^{-1} = e^{-t\A})
    \end{gather*}
\end{proof}
A modo de recordatorio, hemos visto en \ref{obs:sol-gen-fund} que si $\F(t)$ es una matriz fundamental, la solución general de $X' = \A X$ es:
$$
    \F(t) \cdot C\ :\ C = \mx{c_1\\c_2\\\ldots\\c_n} = \sum_{j=1}^d c_jX_j(t)
$$
\begin{eg}[Cálculo de $e^\A t$, la matriz fundamental principal]
    Podemos afrontar el cálculo de diversas formas:
    \begin{itemize}
        \item Pasando a forma canónica de Jordan. (Muy costoso).
        \item Encontrar directamente las $X_1(t), \ldots, X_n(t)$ que forman $\A$. Es decir:
        \begin{enumerate}
            \item Se calculan los autovalores de $\A$, es decir, calcular los $\lambda_i$:
                $$
                    \det(\A-\lambda I) = 0
                $$
                La fórmula anterior también se denomina \textbf{polinomio característico} de la matriz $\A$.
            \item Buscar tantas soluciones de $X'=\A X$ (con datos iniciales apropiados) como el tamaño de $\A$\\
                En caso de que $\lambda$ sea un autovalor con multiplicidad $m$ necesitamos $m$ vectores linealmente independientes asociados a este autovalor. Encontramos todas las soluciones posibles de la forma:
                $$
                    e^{\lambda t} = V
                $$
                donde $V$ es el autovector de $\A$ con autovalor $\lambda$, luego si tenemos multiplicidad sobre $\lambda$, todas las soluciones posibles independiente de la anterior es de la forma: $e^{\lambda t} (V_1 + t V_2)$. Si es necesario se continua con $e^{\lambda t} (V_1 + tV_2 + t^2V_3)$ y así sucesivamente.
        \end{enumerate}
    \end{itemize}
\end{eg}
Vamos a ver distintos usos que podemos darle a la matriz: $e^{t\A}$.
\begin{enumerate}
    \item Se usa como solución del PVI:
    $$
        \begin{cases}
            X' = \A X\\
            X(t_0) = X_0
        \end{cases}
    $$
    Donde la solución general como hemos visto es: $\F(t) C$, entonces:
    $$
        X(t) = e^{t\A} C \implies X_0 = X(t_0) = e^{t_0\A} C \implies C = (e^{t_0 \A})^{-1} X_0 = e^{-t_0\A} X_0 \text{ es decir,}
    $$
    $$
        X(t) = e^{(t-t_0)\A} X_0
    $$
    \item Para encontrar una solución particular de $X' = \A X + B(t)$, por variación de las constantes:
    $$
        X(t) = e^{t\A} U(t) \implies X'(t) = (\Dd{t}e^{t\A})U(t) + e^{t\A} \Dd{t}U(t) = \A e^{t\A} U(t) + e^{t\A}U'(t) \text{ entonces}
    $$
    $$
        \A X + B(t) = X'(t) = \A e^{t\A} U(t) + e^{t\A} U'(t) \implies e^{t\A}U'(t) = B(t)
    $$
    De esta implicación se sigue:
    $$
        U'(t) = e^{-t\A}B(t) \implies U(t) \text{ es una primitiva de } e^{-t\A}B(t)
    $$
    Por ejemplo:
    $$
        U(t) \int_{t_0}^t e^{-s\A}B(s)\d S
    $$
    Y por tanto, la solución particular sería:
    $$
        X(t) = e^{t \A}U(t) = e^{t\A} \int_{t_0}^t e^{-s\A}B(s)\d s
    $$
    \item La solución del PVI:
    $$
        \begin{cases}
            X' = \A X\\
            X(t_0) = X_0
        \end{cases}
    $$ es entonces:
    $$
        X(t) = e^{(t-t_0) \A}X_0 + e^{t\A} \int_{t_0}^t e^{-s\A}B(s)\d s
    $$
    \item Lo mismo se puede hacer para el caso general $X' = \A(t) X + B$.\\
    Si $\F$ es matriz fundamental:
    $$
        \text{PVI}\equiv \begin{cases}
            X' = \A X\\
            X(t_0) = X_0
        \end{cases}
    $$
    entonces:
    \begin{gather}
        X(t) = \F(t) C\\
        X_0(t) = \F(t_0) C\\
        C = \F^{-1}(t_0)X_0 \implies X(t) = \F(t) F(t_0)^{-1} X_0
    \end{gather}
    \item De la misma forma, la solución particular de $X = \A(t) X + B(t)$ es:
    $$
        \F'(t) = \int_{t_0}^{t} \F^{-1}(s) B(s) \d s
    $$
    y la solución del PVI es:
    $$
        X(t) = \F(t)\F^{-1}(t_0)X_0+\F(t)\int_{t_0}^{t} \F^{-1}(s) B(s) \d s
    $$
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Clase del 05/03
\begin{eg}[Cálculo de la matriz fundamental principal $e^{t\A}$]\label{eg:cal-sol-indep-lin-sis}
    Sea:
    $$
        \A = \mx{2 & 0 & 0\\ 0 & 1 & -3\\0 & 3 & 7}
    $$ para el sistema $X' = \A X$.
    \begin{enumerate}
        \item \texttt{Encontramos los autovalores:}\\
        $$
        0 = \det\mx{2-\lambda & 0 & 0\\ 0 & 1-\lambda & -3\\0 & 3 & 7-\lambda} \implies \lambda = 2, \lambda = 4 \text{ (doble)}
        $$
        \item \texttt{Encontramos los autovectores asociados:}\\
        Para $\lambda = 2$, hallamos el autovector $\mx{1 & 0 & 0}^T$, por tanto:
        $$
            X_1(t) = e^{2t}\cdot \mx{1 \\ 0 \\ 0} \text{ es solución de $X' = \A X$}
        $$
        Para $\lambda = 4$, hallamos el autovector resolviendo $V$ para: $(\A - 4I) V = 0$. Obtenemos:
        $$
            X_2(t) = e^{4t}\mx{0 \\ 1 \\ -1}
        $$
        Pero $\lambda = 4$ solo contribuye una solución con autovectores, nos falta otra.
        \item \texttt{Buscamos otras soluciones si es necesario:}\\
        Planteamos una solución del tipo:
        $$
            X = e^{4t}(U_1 + t U_2) \text{ con } U_2 \neq 0 \text{ pues entonces $U_1 = V$}
        $$
        Calculamos $X'$:
        $$
            X' = e^{4t} (4 U_1 + U_2 + 4tU_2) = \A X = \A (e^{4t} (U_1 + t U_2))
        $$
        De esta igualdad se sigue:
        $$
            \begin{cases}
                \A U_1 = 4 U_1 + U_2\\
                \A U_2 = 4 U_2
            \end{cases} \implies
            \begin{cases}
                (\A - 4 I) U_1 = U_2\\
                (\A - 4 I) U_2 = 0
            \end{cases}
        $$
        y como necesitamos que $U_2 \neq 0$, entonces buscamos $U_1$ que cumple:
        $$
            \begin{cases}
                (\A - 4I)^2 U_1 = 0\\
                (\A - 4I) U_1 \neq 0
            \end{cases}
        $$
        Calculamos $(\A - 4I)^2$
        $$
            (\A - 4I)^2 = \mx{4 & 0 & 0 \\ 0 & 0 & 0\\ 0 & 0 & 0}
        $$
        Y entonces cualquier $U_1 = \mx{0 & a & b}^T$ cumple la primera ecuación. Como necesitamos que  $(\A - 4I) U_1 \neq 0$, obtenemos por ejemplo $U_1 = \mx{0 & 1 & 0}$.\\
        Como ya hemos hallado $U_1$ calculamos $U_2 = (\A - 4I) U_1 = \mx{0 & -3 & 3}$ y con esto obtenemos:
        $$
            X_3(t) = e^{4 t} \left( \mx{0 \\ 1 \\ 0} + t \mx{0 \\ -3 \\ 3}\right) = \mx{0 \\ (1-3t) e^{4t} \\ 3t e^{4t}}
        $$
        \item \texttt{Calculamos la matriz fundamental $\F$}
            $$
                \mx{\uparrow && \uparrow\\ X_1(t_0) & X_2(t_0) & X_3(t_0) \\ \downarrow && \downarrow} =
                \mx{e^{2t} & 0 & 0 \\ 0 & e^{4t} & (1-3t) e^{4t} \\ 0 & -e^{4t} & 3te^{4t}}
            $$
            Tendríamos que ver si $e^{t\A} = \F(t)$, que no es cierto pues $\F(0) \neq I$
        \item \texttt{Corección de la matriz fundamental $\F$ para obtener la fundamental principal}
            Como se verá en la proposición \ref{pro:fund-producto}, hay que encontrar $\B$ : $\F(0)\B = I \implies B = \F(0)^{-1}$, entonces:
            $$
                \B = \F{0}^{1} = \mx{1 & 0 & 0\\0 & 0 & -1 \\ 0 & -1 & 1}
            $$
            Y finalmente:
            $$
                e^{t\A} = \F(t)\B = \mx{e^{2t} & 0 & 0\\ 0 & (1-3t)e^{4t} & -3te^{4t} \\ 0 & 3te^{4t} & (1+3t)e^{4t}}
            $$
            Es fácil comprobar que esta última matriz es $I$ con $t=0$.
    \end{enumerate}
\end{eg}
\begin{obs}
    Si en el ejemplo \ref{eg:cal-sol-indep-lin-sis} estuviésemos en dimensión más alta buscaríamos todas las soluciones del tipo:
    $$
        e^{4t}(U_1 + tU_2) : U_2 \neq 0
    $$
    So no hay tantas soluciones a la ecuación anterior como multiplicidad del autovalor, buscamos el mayor número posible de la forma:
    $$
        e^{4t}(U_1 + tU_2 + \frac{t^2}{2!}U_3) : U_3 \neq 0
    $$
    y así sucesivamente.
\end{obs}

\begin{pro}[Matriz fundamental como producto de una fundamental y otra matriz]\label{pro:fund-producto}
    Si $\F(t)$ es una matriz fundamental y $\B$ es invertible, entonces $\F(t) B$ también es una matriz fundamental.
\end{pro}
\begin{proof}
    $$
        (\F(t)\B)' = \F'(t)B = (\A \F(t)) \B = \A(\F(t) \B) \implies \text{las columnas de $\F(t)\B$ son soluciones}
    $$
    y además,
    $$
        \F(t_0) \text{ invertible} \implies \F(t_0)\B \text{ es invertible}
    $$
\end{proof}

\begin{pro}[EDO que satisface el determinante de una matriz de soluciones]
    Sea $X' = \A(t) X$ con $\A$ de tamaño $d \times d$.\\
    Sean $X_1, \ldots, X_d$ soluciones no necesariamente linealmente independientes, $w(t) = \det\F$ con $\F(t)$:
    $$
        \F(t) =
            \mx{\uparrow && \uparrow\\ X_1(t_0) & \cdots & X_d(t_0) \\ \downarrow && \downarrow}
    $$
    entonces, $w(t)$ satisface la EDO $w'(t) = a(t) w(t)$ donde $a(t) = \traza(\A(t))$.\\
    Como consecuencia:
    \begin{gather*}
        w(t) = w(t_0) e^{\int_{t_0}^t a(s) \d s}\\
        w(t) = 0 \text{ (en su intervalo de definición), o bien } w(t) \neq 0
    \end{gather*}
\end{pro}
\begin{proof}
    La demostración sigue de forma directa, desarrollando la derivada del $\det\F$ y sabiendo que $\F' = \A \F$. Terminamos hallando que $\Dd{t} \det \F = \left(\sum a_{ii}\right) w(t)$
\end{proof}
